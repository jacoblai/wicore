"""
è¯·æ±‚è°ƒåº¦å™¨ - WiCore Mojo æ¨ç†å¼•æ“
å®ç°æ™ºèƒ½æ‰¹å¤„ç†è°ƒåº¦ã€ä¼˜å…ˆçº§ç®¡ç†å’Œè´Ÿè½½å‡è¡¡
æ”¯æŒå¼‚æ­¥å¤„ç†å’Œæµå¼è¾“å‡º
"""

from collections import Dict, List, Deque
from python import Python
from .model_executor import ModelExecutor
import time
import threading

enum RequestPriority:
    LOW = 0
    NORMAL = 1 
    HIGH = 2
    URGENT = 3

enum RequestStatus:
    PENDING = 0
    PROCESSING = 1
    COMPLETED = 2
    FAILED = 3
    CANCELLED = 4

struct InferenceRequest:
    """æ¨ç†è¯·æ±‚ç»“æ„ä½“"""
    var request_id: String
    var input_text: String
    var max_tokens: Int
    var temperature: Float64
    var priority: RequestPriority
    var created_time: Float64
    var timeout_seconds: Int
    var stream: Bool
    var status: RequestStatus
    var result: String
    var error_message: String
    var processing_start_time: Float64
    var processing_end_time: Float64
    
    fn __init__(inout self):
        """é»˜è®¤åˆå§‹åŒ–"""
        self.request_id = ""
        self.input_text = ""
        self.max_tokens = 512
        self.temperature = 0.7
        self.priority = RequestPriority.NORMAL
        self.created_time = time.time_ns() / 1e9
        self.timeout_seconds = 30
        self.stream = False
        self.status = RequestStatus.PENDING
        self.result = ""
        self.error_message = ""
        self.processing_start_time = 0.0
        self.processing_end_time = 0.0
    
    fn __init__(inout self, request_id: String, input_text: String):
        """åŸºç¡€åˆå§‹åŒ–"""
        self.request_id = request_id
        self.input_text = input_text
        self.max_tokens = 512
        self.temperature = 0.7
        self.priority = RequestPriority.NORMAL
        self.created_time = time.time_ns() / 1e9
        self.timeout_seconds = 30
        self.stream = False
        self.status = RequestStatus.PENDING
        self.result = ""
        self.error_message = ""
        self.processing_start_time = 0.0
        self.processing_end_time = 0.0
    
    fn is_expired(self) -> Bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦è¿‡æœŸ"""
        current_time = time.time_ns() / 1e9
        return (current_time - self.created_time) > Float64(self.timeout_seconds)
    
    fn get_wait_time(self) -> Float64:
        """è·å–ç­‰å¾…æ—¶é—´"""
        current_time = time.time_ns() / 1e9
        if self.processing_start_time > 0:
            return self.processing_start_time - self.created_time
        else:
            return current_time - self.created_time
    
    fn get_processing_time(self) -> Float64:
        """è·å–å¤„ç†æ—¶é—´"""
        if self.processing_start_time == 0:
            return 0.0
        
        end_time = self.processing_end_time if self.processing_end_time > 0 else time.time_ns() / 1e9
        return end_time - self.processing_start_time
    
    fn mark_processing(inout self):
        """æ ‡è®°ä¸ºå¤„ç†ä¸­"""
        self.status = RequestStatus.PROCESSING
        self.processing_start_time = time.time_ns() / 1e9
    
    fn mark_completed(inout self, result: String):
        """æ ‡è®°ä¸ºå®Œæˆ"""
        self.status = RequestStatus.COMPLETED
        self.result = result
        self.processing_end_time = time.time_ns() / 1e9
    
    fn mark_failed(inout self, error: String):
        """æ ‡è®°ä¸ºå¤±è´¥"""
        self.status = RequestStatus.FAILED
        self.error_message = error
        self.processing_end_time = time.time_ns() / 1e9


struct BatchProcessor:
    """æ‰¹å¤„ç†å™¨"""
    var max_batch_size: Int
    var max_wait_time: Float64      # æœ€å¤§ç­‰å¾…æ—¶é—´(ç§’)
    var current_batch: List[InferenceRequest]
    var batch_start_time: Float64
    
    fn __init__(inout self, max_batch_size: Int, max_wait_time: Float64 = 0.1):
        """åˆå§‹åŒ–æ‰¹å¤„ç†å™¨"""
        self.max_batch_size = max_batch_size
        self.max_wait_time = max_wait_time
        self.current_batch = List[InferenceRequest]()
        self.batch_start_time = 0.0
    
    fn add_request(inout self, request: InferenceRequest) -> Bool:
        """æ·»åŠ è¯·æ±‚åˆ°å½“å‰æ‰¹æ¬¡"""
        if len(self.current_batch) >= self.max_batch_size:
            return False
        
        if len(self.current_batch) == 0:
            self.batch_start_time = time.time_ns() / 1e9
        
        self.current_batch.append(request)
        return True
    
    fn should_process_batch(self) -> Bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†å½“å‰æ‰¹æ¬¡"""
        if len(self.current_batch) == 0:
            return False
        
        # æ‰¹æ¬¡å·²æ»¡
        if len(self.current_batch) >= self.max_batch_size:
            return True
        
        # ç­‰å¾…æ—¶é—´è¶…è¿‡é˜ˆå€¼
        current_time = time.time_ns() / 1e9
        if (current_time - self.batch_start_time) > self.max_wait_time:
            return True
        
        return False
    
    fn get_batch(inout self) -> List[InferenceRequest]:
        """è·å–å½“å‰æ‰¹æ¬¡å¹¶é‡ç½®"""
        batch = self.current_batch
        self.current_batch = List[InferenceRequest]()
        self.batch_start_time = 0.0
        return batch


struct SchedulerStats:
    """è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯"""
    var total_requests: Int
    var completed_requests: Int
    var failed_requests: Int
    var cancelled_requests: Int
    var total_processing_time: Float64
    var total_wait_time: Float64
    var max_queue_length: Int
    var current_queue_length: Int
    
    fn __init__(inout self):
        """åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        self.total_requests = 0
        self.completed_requests = 0
        self.failed_requests = 0
        self.cancelled_requests = 0
        self.total_processing_time = 0.0
        self.total_wait_time = 0.0
        self.max_queue_length = 0
        self.current_queue_length = 0
    
    fn update_request_completed(inout self, request: InferenceRequest):
        """æ›´æ–°è¯·æ±‚å®Œæˆç»Ÿè®¡"""
        self.completed_requests += 1
        self.total_processing_time += request.get_processing_time()
        self.total_wait_time += request.get_wait_time()
    
    fn update_request_failed(inout self):
        """æ›´æ–°è¯·æ±‚å¤±è´¥ç»Ÿè®¡"""
        self.failed_requests += 1
    
    fn update_queue_length(inout self, length: Int):
        """æ›´æ–°é˜Ÿåˆ—é•¿åº¦ç»Ÿè®¡"""
        self.current_queue_length = length
        if length > self.max_queue_length:
            self.max_queue_length = length
    
    fn get_average_processing_time(self) -> Float64:
        """è·å–å¹³å‡å¤„ç†æ—¶é—´"""
        if self.completed_requests == 0:
            return 0.0
        return self.total_processing_time / Float64(self.completed_requests)
    
    fn get_average_wait_time(self) -> Float64:
        """è·å–å¹³å‡ç­‰å¾…æ—¶é—´"""
        if self.completed_requests == 0:
            return 0.0
        return self.total_wait_time / Float64(self.completed_requests)
    
    fn get_success_rate(self) -> Float64:
        """è·å–æˆåŠŸç‡"""
        if self.total_requests == 0:
            return 0.0
        return Float64(self.completed_requests) / Float64(self.total_requests) * 100.0


struct RequestScheduler:
    """è¯·æ±‚è°ƒåº¦å™¨ä¸»ç±»"""
    var model_executor: ModelExecutor
    var config: WiCoreConfig
    var request_queues: List[Deque[InferenceRequest]]  # æŒ‰ä¼˜å…ˆçº§åˆ†é˜Ÿåˆ—
    var active_requests: Dict[String, InferenceRequest]
    var batch_processor: BatchProcessor
    var stats: SchedulerStats
    var running: Bool
    var max_concurrent_requests: Int
    
    fn __init__(inout self, model_executor: ModelExecutor, config: WiCoreConfig):
        """åˆå§‹åŒ–è¯·æ±‚è°ƒåº¦å™¨"""
        print("ğŸ“‹ åˆå§‹åŒ–è¯·æ±‚è°ƒåº¦å™¨...")
        
        self.model_executor = model_executor
        self.config = config
        self.running = False
        self.max_concurrent_requests = config.max_batch_size * 2
        
        # åˆå§‹åŒ–ä¼˜å…ˆçº§é˜Ÿåˆ—
        self.request_queues = List[Deque[InferenceRequest]]()
        for _ in range(4):  # 4ä¸ªä¼˜å…ˆçº§
            self.request_queues.append(Deque[InferenceRequest]())
        
        self.active_requests = Dict[String, InferenceRequest]()
        
        # åˆå§‹åŒ–æ‰¹å¤„ç†å™¨
        self.batch_processor = BatchProcessor(config.max_batch_size, 0.05)  # 50msç­‰å¾…
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats = SchedulerStats()
        
        print("âœ… è¯·æ±‚è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    fn start(inout self) -> Bool:
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if self.running:
            print("âš ï¸  è°ƒåº¦å™¨å·²åœ¨è¿è¡Œ")
            return True
        
        print("ğŸš€ å¯åŠ¨è¯·æ±‚è°ƒåº¦å™¨...")
        
        self.running = True
        
        # åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨è°ƒåº¦çº¿ç¨‹
        # ç®€åŒ–å®ç°ï¼šæ ‡è®°ä¸ºè¿è¡ŒçŠ¶æ€
        print("âœ… è¯·æ±‚è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸ")
        return True
    
    fn stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        if not self.running:
            return
        
        print("ğŸ›‘ åœæ­¢è¯·æ±‚è°ƒåº¦å™¨...")
        
        self.running = False
        
        # å¤„ç†å‰©ä½™è¯·æ±‚
        self._process_remaining_requests()
        
        print("âœ… è¯·æ±‚è°ƒåº¦å™¨å·²åœæ­¢")
    
    fn submit_request(inout self, request: InferenceRequest) -> String:
        """æäº¤æ¨ç†è¯·æ±‚"""
        if not self.running:
            return "Error: Scheduler not running"
        
        # æ£€æŸ¥å¹¶å‘é™åˆ¶
        if len(self.active_requests) >= self.max_concurrent_requests:
            return "Error: Too many concurrent requests"
        
        # ç”Ÿæˆè¯·æ±‚ID
        if request.request_id == "":
            request.request_id = self._generate_request_id()
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats.total_requests += 1
        
        # æ ¹æ®ä¼˜å…ˆçº§åŠ å…¥ç›¸åº”é˜Ÿåˆ—
        priority_index = int(request.priority)
        self.request_queues[priority_index].append(request)
        
        # æ›´æ–°é˜Ÿåˆ—é•¿åº¦ç»Ÿè®¡
        total_queue_length = self._get_total_queue_length()
        self.stats.update_queue_length(total_queue_length)
        
        print(f"ğŸ“ æäº¤è¯·æ±‚: {request.request_id} (ä¼˜å…ˆçº§: {priority_index})")
        
        # å°è¯•ç«‹å³å¤„ç†
        self._try_process_requests()
        
        return request.request_id
    
    fn get_request_status(self, request_id: String) -> Optional[InferenceRequest]:
        """è·å–è¯·æ±‚çŠ¶æ€"""
        # æ£€æŸ¥æ´»è·ƒè¯·æ±‚
        if request_id in self.active_requests:
            return self.active_requests[request_id]
        
        # æ£€æŸ¥é˜Ÿåˆ—ä¸­çš„è¯·æ±‚
        for queue in self.request_queues:
            for request in queue:
                if request[].request_id == request_id:
                    return request[]
        
        return None
    
    fn cancel_request(inout self, request_id: String) -> Bool:
        """å–æ¶ˆè¯·æ±‚"""
        # ä»æ´»è·ƒè¯·æ±‚ä¸­ç§»é™¤
        if request_id in self.active_requests:
            request = self.active_requests[request_id]
            request.status = RequestStatus.CANCELLED
            del self.active_requests[request_id]
            self.stats.cancelled_requests += 1
            print(f"âŒ å–æ¶ˆæ´»è·ƒè¯·æ±‚: {request_id}")
            return True
        
        # ä»é˜Ÿåˆ—ä¸­ç§»é™¤
        for queue in self.request_queues:
            for i in range(len(queue)):
                if queue[i].request_id == request_id:
                    request = queue[i]
                    request.status = RequestStatus.CANCELLED
                    # è¿™é‡Œéœ€è¦ä»é˜Ÿåˆ—ä¸­åˆ é™¤å…ƒç´ 
                    # ç®€åŒ–å®ç°ï¼šæ ‡è®°ä¸ºå–æ¶ˆ
                    self.stats.cancelled_requests += 1
                    print(f"âŒ å–æ¶ˆé˜Ÿåˆ—è¯·æ±‚: {request_id}")
                    return True
        
        return False
    
    fn _try_process_requests(inout self):
        """å°è¯•å¤„ç†è¯·æ±‚"""
        if not self.running:
            return
        
        # åˆ›å»ºæ‰¹æ¬¡
        batch = self._create_optimal_batch()
        if len(batch) == 0:
            return
        
        # å¤„ç†æ‰¹æ¬¡
        self._process_batch(batch)
    
    fn _create_optimal_batch(inout self) -> List[InferenceRequest]:
        """åˆ›å»ºæœ€ä¼˜æ‰¹æ¬¡"""
        batch = List[InferenceRequest]()
        
        # ä»é«˜ä¼˜å…ˆçº§åˆ°ä½ä¼˜å…ˆçº§é€‰æ‹©è¯·æ±‚
        for priority in range(3, -1, -1):
            queue = self.request_queues[priority]
            
            while len(queue) > 0 and len(batch) < self.config.max_batch_size:
                request = queue.popleft()
                
                # æ£€æŸ¥è¯·æ±‚æ˜¯å¦è¿‡æœŸ
                if request.is_expired():
                    request.status = RequestStatus.FAILED
                    request.error_message = "Request timeout"
                    self.stats.update_request_failed()
                    continue
                
                batch.append(request)
        
        return batch
    
    fn _process_batch(inout self, batch: List[InferenceRequest]):
        """å¤„ç†æ‰¹æ¬¡"""
        if len(batch) == 0:
            return
        
        print(f"âš¡ å¤„ç†æ‰¹æ¬¡: {len(batch)} ä¸ªè¯·æ±‚")
        
        # æ ‡è®°è¯·æ±‚ä¸ºå¤„ç†ä¸­
        for request in batch:
            request[].mark_processing()
            self.active_requests[request[].request_id] = request[]
        
        # æ‰§è¡Œæ‰¹é‡æ¨ç†
        batch_inputs = List[String]()
        for request in batch:
            batch_inputs.append(request[].input_text)
        
        try:
            # è°ƒç”¨æ¨¡å‹æ‰§è¡Œå™¨
            batch_results = self.model_executor.infer_batch(batch_inputs)
            
            # å¤„ç†ç»“æœ
            for i in range(len(batch)):
                request = batch[i]
                
                if i < len(batch_results):
                    result = batch_results[i]
                    request[].mark_completed(result)
                    self.stats.update_request_completed(request[])
                    print(f"âœ… å®Œæˆè¯·æ±‚: {request[].request_id}")
                else:
                    request[].mark_failed("Inference failed")
                    self.stats.update_request_failed()
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {request[].request_id}")
                
                # ä»æ´»è·ƒè¯·æ±‚ä¸­ç§»é™¤
                if request[].request_id in self.active_requests:
                    del self.active_requests[request[].request_id]
        
        except Exception as e:
            # å¤„ç†æ‰¹æ¬¡å¤±è´¥
            print("âŒ æ‰¹æ¬¡å¤„ç†å¤±è´¥:", str(e))
            
            for request in batch:
                request[].mark_failed(str(e))
                self.stats.update_request_failed()
                
                if request[].request_id in self.active_requests:
                    del self.active_requests[request[].request_id]
    
    fn _process_remaining_requests(inout self):
        """å¤„ç†å‰©ä½™è¯·æ±‚"""
        print("ğŸ”„ å¤„ç†å‰©ä½™è¯·æ±‚...")
        
        # å–æ¶ˆæ‰€æœ‰é˜Ÿåˆ—ä¸­çš„è¯·æ±‚
        for queue in self.request_queues:
            while len(queue) > 0:
                request = queue.popleft()
                request.status = RequestStatus.CANCELLED
                request.error_message = "Scheduler shutdown"
                self.stats.cancelled_requests += 1
        
        # å–æ¶ˆæ‰€æœ‰æ´»è·ƒè¯·æ±‚
        for request_id in self.active_requests:
            request = self.active_requests[request_id]
            request.status = RequestStatus.CANCELLED
            request.error_message = "Scheduler shutdown"
            self.stats.cancelled_requests += 1
        
        self.active_requests.clear()
    
    fn _get_total_queue_length(self) -> Int:
        """è·å–æ€»é˜Ÿåˆ—é•¿åº¦"""
        total = 0
        for queue in self.request_queues:
            total += len(queue)
        return total
    
    fn _generate_request_id(self) -> String:
        """ç”Ÿæˆè¯·æ±‚ID"""
        timestamp = time.time_ns()
        return "req_" + str(timestamp)
    
    fn get_scheduler_status(self) -> String:
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        status = f"è°ƒåº¦å™¨çŠ¶æ€:\\n"
        status += f"  è¿è¡Œä¸­: {'æ˜¯' if self.running else 'å¦'}\\n"
        status += f"  æ€»è¯·æ±‚æ•°: {self.stats.total_requests}\\n"
        status += f"  å·²å®Œæˆ: {self.stats.completed_requests}\\n"
        status += f"  å¤±è´¥: {self.stats.failed_requests}\\n"
        status += f"  å–æ¶ˆ: {self.stats.cancelled_requests}\\n"
        status += f"  æ´»è·ƒè¯·æ±‚: {len(self.active_requests)}\\n"
        status += f"  é˜Ÿåˆ—é•¿åº¦: {self._get_total_queue_length()}\\n"
        status += f"  æˆåŠŸç‡: {self.stats.get_success_rate():.1f}%\\n"
        status += f"  å¹³å‡å¤„ç†æ—¶é—´: {self.stats.get_average_processing_time():.3f}s\\n"
        status += f"  å¹³å‡ç­‰å¾…æ—¶é—´: {self.stats.get_average_wait_time():.3f}s"
        
        return status
    
    fn get_queue_summary(self) -> String:
        """è·å–é˜Ÿåˆ—æ‘˜è¦"""
        summary = "é˜Ÿåˆ—çŠ¶æ€:\\n"
        
        priority_names = ["ä½", "æ™®é€š", "é«˜", "ç´§æ€¥"]
        for i in range(4):
            queue_length = len(self.request_queues[i])
            summary += f"  {priority_names[i]}ä¼˜å…ˆçº§: {queue_length} ä¸ªè¯·æ±‚\\n"
        
        summary += f"  æ´»è·ƒå¤„ç†: {len(self.active_requests)} ä¸ªè¯·æ±‚"
        return summary 