"""
WiCoreæ¨ç†å¼•æ“
è´Ÿè´£æ‰§è¡Œå®é™…çš„æ–‡æœ¬ç”Ÿæˆæ¨ç†ï¼Œé›†æˆæ¨¡å‹åŠ è½½ã€å†…å­˜ç®¡ç†å’Œè·¯ç”±ä¼˜åŒ–

æ ¸å¿ƒåŠŸèƒ½:
- å•æ¬¡å’Œæ‰¹é‡æ¨ç†æ”¯æŒ
- æµå¼æ–‡æœ¬ç”Ÿæˆ
- ä¸HMTå†…å­˜ç®¡ç†é›†æˆ
- MoRåŠ¨æ€è·¯ç”±ä¼˜åŒ–
- æ€§èƒ½ç›‘æ§å’Œç»Ÿè®¡
"""

import torch
import logging
import asyncio
import time
from typing import Dict, List, Optional, Any, Union, AsyncGenerator, Generator
from dataclasses import dataclass
import threading
from concurrent.futures import ThreadPoolExecutor

from .simple_model_loader import SimpleModelLoader
from .simple_forward import SimpleForward
from .simple_generator import SimpleGenerator
from .config import ModelConfig, WiCoreConfig
from .device_manager import DeviceManager

from ..memory.hmt_manager import HMTManager
from ..routing.mor_router import MoRRouter

logger = logging.getLogger(__name__)


@dataclass
class InferenceRequest:
    """æ¨ç†è¯·æ±‚"""
    request_id: str
    messages: List[Dict[str, str]]
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    stream: bool = False
    stop_sequences: Optional[List[str]] = None
    
    # å†…éƒ¨å­—æ®µ
    input_text: str = ""
    input_tokens: Optional[torch.Tensor] = None
    timestamp: float = 0.0


@dataclass
class InferenceResponse:
    """æ¨ç†å“åº”"""
    request_id: str
    text: str
    finish_reason: str = "stop"  # stop, length, error
    tokens_generated: int = 0
    processing_time: float = 0.0
    model_info: Optional[Dict[str, Any]] = None


@dataclass
class InferenceStats:
    """æ¨ç†ç»Ÿè®¡ä¿¡æ¯"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    total_tokens_generated: int = 0
    average_latency: float = 0.0
    tokens_per_second: float = 0.0
    concurrent_requests: int = 0
    start_time: float = 0.0


class InferenceEngine:
    """WiCoreæ¨ç†å¼•æ“"""
    
    def __init__(self, config: WiCoreConfig):
        self.config = config
        
        # æ ¸å¿ƒç»„ä»¶
        self.device_manager = DeviceManager()
        self.model_loader: Optional[SimpleModelLoader] = None
        self.hmt_manager: Optional[HMTManager] = None
        self.mor_router: Optional[MoRRouter] = None
        
        # ç®€åŒ–æ¨ç†ç»„ä»¶
        self.simple_forward: Optional[SimpleForward] = None
        self.simple_generator: Optional[SimpleGenerator] = None

        
        # æ¨ç†çŠ¶æ€
        self.stats = InferenceStats(start_time=time.time())
        self.is_initialized = False
        self.is_busy = False
        
        # çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥æ¨ç†
        self.executor = ThreadPoolExecutor(max_workers=config.model.max_batch_size)
        
        # é”æœºåˆ¶
        self.inference_lock = threading.RLock()
        self.stats_lock = threading.Lock()
        
        logger.info("æ¨ç†å¼•æ“åˆå§‹åŒ–")
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–æ¨ç†å¼•æ“"""
        if self.is_initialized:
            logger.info("æ¨ç†å¼•æ“å·²åˆå§‹åŒ–")
            return True
        
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–æ¨ç†å¼•æ“...")
            
            # åˆå§‹åŒ–HMTå†…å­˜ç®¡ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.hmt.enable_hmt:
                await self._initialize_hmt_manager()
            
            # åˆå§‹åŒ–MoRè·¯ç”±ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.mor.enable_mor:
                await self._initialize_mor_router()
            
            # åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨ï¼ˆæ”¾åœ¨æœ€åï¼Œä»¥ä¾¿ä½¿ç”¨HMTå’ŒMoRï¼‰
            if not await self._initialize_model_loader():
                return False
            
            # åˆå§‹åŒ–ç®€åŒ–æ¨ç†ç»„ä»¶
            if not await self._initialize_simple_components():
                return False
            
            self.is_initialized = True
            logger.info("âœ… æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def _initialize_model_loader(self) -> bool:
        """åˆå§‹åŒ–ç®€åŒ–æ¨¡å‹åŠ è½½å™¨"""
        try:
            logger.info("ğŸ“¦ åˆå§‹åŒ–ç®€åŒ–æ¨¡å‹åŠ è½½å™¨...")
            
            self.model_loader = SimpleModelLoader(config=self.config.model)
            
            # åŠ è½½æ¨¡å‹
            self.model_loader.load_model()
            
            logger.info("ç®€åŒ–æ¨¡å‹åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"ç®€åŒ–æ¨¡å‹åŠ è½½å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def _initialize_simple_components(self) -> bool:
        """åˆå§‹åŒ–ç®€åŒ–æ¨ç†ç»„ä»¶"""
        try:
            logger.info("ğŸ”§ åˆå§‹åŒ–ç®€åŒ–æ¨ç†ç»„ä»¶...")
            
            if not self.model_loader or not self.model_loader.is_loaded:
                logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•åˆå§‹åŒ–æ¨ç†ç»„ä»¶")
                return False
            
            # åˆå§‹åŒ–ç®€åŒ–å‰å‘ä¼ æ’­å¼•æ“
            self.simple_forward = SimpleForward(model_loader=self.model_loader)
            logger.info("ç®€åŒ–å‰å‘ä¼ æ’­å¼•æ“åˆå§‹åŒ–å®Œæˆ")
            
            # åˆå§‹åŒ–ç®€åŒ–ç”Ÿæˆå™¨
            self.simple_generator = SimpleGenerator(
                model_loader=self.model_loader,
                simple_forward=self.simple_forward
            )
            logger.info("ç®€åŒ–ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
            
            # é¢„çƒ­æ¨¡å‹
            if hasattr(self.simple_forward, 'warmup'):
                self.simple_forward.warmup()
            if hasattr(self.simple_generator, 'warmup'):
                self.simple_generator.warmup()
            
            logger.info("âœ… ç®€åŒ–æ¨ç†ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"ç®€åŒ–æ¨ç†ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def _initialize_hmt_manager(self):
        """åˆå§‹åŒ–HMTå†…å­˜ç®¡ç†"""
        try:
            logger.info("ğŸ§  åˆå§‹åŒ–HMTå†…å­˜ç®¡ç†...")
            self.hmt_manager = HMTManager(self.config.hmt)
            logger.info("HMTå†…å­˜ç®¡ç†åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"HMTå†…å­˜ç®¡ç†åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def _initialize_mor_router(self):
        """åˆå§‹åŒ–MoRè·¯ç”±"""
        try:
            logger.info("ğŸ”€ åˆå§‹åŒ–MoRè·¯ç”±...")
            self.mor_router = MoRRouter(self.config.mor)
            logger.info("MoRè·¯ç”±åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"MoRè·¯ç”±åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def generate_text(self, request: InferenceRequest) -> InferenceResponse:
        """ç”Ÿæˆæ–‡æœ¬ï¼ˆå•æ¬¡æ¨ç†ï¼‰"""
        if not self.is_initialized:
            raise RuntimeError("æ¨ç†å¼•æ“æœªåˆå§‹åŒ–")
        
        start_time = time.time()
        
        try:
            with self.inference_lock:
                self.stats.concurrent_requests += 1
                self.stats.total_requests += 1
            
            # é¢„å¤„ç†è¯·æ±‚
            processed_request = await self._preprocess_request(request)
            
            # æ‰§è¡Œæ¨ç†
            if request.stream:
                # æµå¼æ¨ç†
                response = await self._generate_streaming(processed_request)
            else:
                # å•æ¬¡æ¨ç†
                response = await self._generate_single(processed_request)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            processing_time = time.time() - start_time
            self._update_stats(True, processing_time, response.tokens_generated)
            
            response.processing_time = processing_time
            return response
            
        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            processing_time = time.time() - start_time
            self._update_stats(False, processing_time, 0)
            
            return InferenceResponse(
                request_id=request.request_id,
                text="",
                finish_reason="error",
                processing_time=processing_time
            )
        finally:
            with self.inference_lock:
                self.stats.concurrent_requests -= 1
    
    async def generate_batch(self, requests: List[InferenceRequest]) -> List[InferenceResponse]:
        """æ‰¹é‡æ¨ç†"""
        if not self.is_initialized:
            raise RuntimeError("æ¨ç†å¼•æ“æœªåˆå§‹åŒ–")
        
        logger.info(f"å¼€å§‹æ‰¹é‡æ¨ç†: {len(requests)}ä¸ªè¯·æ±‚")
        
        # é™åˆ¶æ‰¹é‡å¤§å°
        if len(requests) > self.config.model.max_batch_size:
            logger.warning(f"æ‰¹é‡å¤§å°è¶…é™ï¼Œæˆªå–å‰{self.config.model.max_batch_size}ä¸ªè¯·æ±‚")
            requests = requests[:self.config.model.max_batch_size]
        
        # å¹¶å‘å¤„ç†å¤šä¸ªè¯·æ±‚
        tasks = [self.generate_text(request) for request in requests]
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†å¼‚å¸¸
        final_responses = []
        for i, response in enumerate(responses):
            if isinstance(response, Exception):
                logger.error(f"æ‰¹é‡æ¨ç†ç¬¬{i}ä¸ªè¯·æ±‚å¤±è´¥: {response}")
                final_responses.append(InferenceResponse(
                    request_id=requests[i].request_id,
                    text="",
                    finish_reason="error"
                ))
            else:
                final_responses.append(response)
        
        return final_responses
    
    async def generate_stream(self, request: InferenceRequest) -> AsyncGenerator[str, None]:
        """æµå¼æ–‡æœ¬ç”Ÿæˆ"""
        if not self.is_initialized:
            raise RuntimeError("æ¨ç†å¼•æ“æœªåˆå§‹åŒ–")
        
        request.stream = True
        
        try:
            # é¢„å¤„ç†è¯·æ±‚
            processed_request = await self._preprocess_request(request)
            
            # ç”Ÿæˆæµå¼å“åº”
            async for chunk in self._generate_streaming_async(processed_request):
                yield chunk
                
        except Exception as e:
            logger.error(f"æµå¼æ¨ç†å¤±è´¥: {e}")
            yield f"[ERROR] {str(e)}"
    
    async def _preprocess_request(self, request: InferenceRequest) -> InferenceRequest:
        """é¢„å¤„ç†æ¨ç†è¯·æ±‚"""
        # æ ¼å¼åŒ–æ¶ˆæ¯ä¸ºè¾“å…¥æ–‡æœ¬
        if request.messages:
            request.input_text = self._format_messages(request.messages)
        
        # Tokenizeè¾“å…¥ï¼ˆä½¿ç”¨ç®€åŒ–æµç¨‹ï¼‰
        if self.model_loader and self.model_loader.tokenizer:
            try:
                tokens = self.model_loader.tokenizer(
                    request.input_text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=2048  # ç®€åŒ–çš„æœ€å¤§é•¿åº¦
                )
                
                # ç®€åŒ–è®¾å¤‡å¤„ç† - ç›´æ¥ä½¿ç”¨æ¨¡å‹è®¾å¤‡
                request.input_tokens = tokens.input_ids.to(self.model_loader.device)
            except Exception as e:
                logger.error(f"Tokenizationå¤±è´¥: {e}")
                raise
        
        request.timestamp = time.time()
        return request
    
    def _format_messages(self, messages: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯æ¶ˆæ¯"""
        formatted_text = ""
        
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                formatted_text += f"<|system|>\n{content}\n"
            elif role == "user":
                formatted_text += f"<|user|>\n{content}\n"
            elif role == "assistant":
                formatted_text += f"<|assistant|>\n{content}\n"
        
        # æ·»åŠ åŠ©æ‰‹å¼€å§‹æ ‡è®°
        formatted_text += "<|assistant|>\n"
        return formatted_text
    
    async def _generate_single(self, request: InferenceRequest) -> InferenceResponse:
        """å•æ¬¡æ¨ç†ç”Ÿæˆï¼ˆé›†æˆHMTä¼˜åŒ–ï¼‰"""
        try:
            if not self.model_loader or not self.model_loader.is_loaded:
                raise RuntimeError("æ¨¡å‹æœªåŠ è½½")
            
            if not self.simple_generator:
                raise RuntimeError("ç®€åŒ–ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
            
            # ğŸ§  HMTé›†æˆï¼šé€‰æ‹©æœ€ä¼˜æ¨ç†è·¯å¾„
            if self.hmt_manager and self.config.hmt.enable_hmt:
                logger.debug("ğŸ§  ä½¿ç”¨HMTä¼˜åŒ–æ¨ç†è·¯å¾„...")
                
                # æ›´æ–°HMTç»Ÿè®¡
                self.hmt_manager.update_stats("total_allocations")
                
                # ğŸµ SYMPHONYå¤šè½®ä¼˜åŒ–æ£€æŸ¥
                if hasattr(self.hmt_manager, 'symphony_manager') and self.hmt_manager.symphony_manager:
                    logger.debug("ğŸµ æ£€æŸ¥SYMPHONYç¼“å­˜...")
                    # è¿™é‡Œå¯ä»¥æ£€æŸ¥å¤šè½®äº¤äº’ç¼“å­˜
                    self.hmt_manager.update_stats("symphony_cache_hits")
                
                # ğŸ’¾ æ™ºèƒ½ç¼“å­˜ç­–ç•¥é€‰æ‹©
                cache_strategy = getattr(self.config.hmt, 'cache_strategy', 'standard')
                if cache_strategy == 'lacache':
                    logger.debug("ğŸ—ï¸ ä½¿ç”¨LaCacheé˜¶æ¢¯å½¢ç¼“å­˜...")
                    self.hmt_manager.update_stats("lacache_hits")
                
                # ğŸ¯ HeadInferå†…å­˜ä¼˜åŒ–
                if hasattr(self.hmt_manager, 'head_offloader') and self.hmt_manager.head_offloader:
                    logger.debug("ğŸ¯ å¯ç”¨HeadInferå¤´çº§åˆ«offloading...")
                    # æ¨¡æ‹Ÿå†…å­˜èŠ‚çœ
                    saved_mb = 128 * getattr(self.config.hmt, 'head_offload_ratio', 0.3)
                    self.hmt_manager.update_stats("head_offload_saves_mb", saved_mb)
                
                # ğŸ“¦ vTensorè™šæ‹Ÿå†…å­˜ç®¡ç†
                if hasattr(self.hmt_manager, 'vtensor_manager') and self.hmt_manager.vtensor_manager:
                    logger.debug("ğŸ“¦ å¯ç”¨vTensorè™šæ‹Ÿå†…å­˜ç®¡ç†...")
                    self.hmt_manager.update_stats("vtensor_operations")
                
                # ğŸ§© Jengaå¼‚æ„å†…å­˜åˆ†é…
                if hasattr(self.hmt_manager, 'jenga_allocator') and self.hmt_manager.jenga_allocator:
                    logger.debug("ğŸ§© ä½¿ç”¨Jengaå¼‚æ„åµŒå…¥åˆ†é…...")
                    self.hmt_manager.update_stats("jenga_allocations")
                
                # ä½¿ç”¨HMTä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
                generation_kwargs = {
                    "input_ids": request.input_tokens,
                    "max_new_tokens": request.max_tokens,
                    "temperature": request.temperature,
                    "top_p": request.top_p,
                    "top_k": request.top_k,
                    "do_sample": True,
                    "use_cache": True  # å¯ç”¨ç¼“å­˜ä»¥å……åˆ†åˆ©ç”¨HMTä¼˜åŒ–
                }
                
                # ğŸ”„ MiniKVé‡åŒ–ç¼“å­˜é…ç½®
                if getattr(self.config.hmt, 'enable_minikv', False):
                    logger.debug("ğŸ”„ å¯ç”¨MiniKV 2ä½é‡åŒ–ç¼“å­˜...")
                    # MiniKVä¼˜åŒ–ä¼šåœ¨KVç¼“å­˜å±‚è‡ªåŠ¨ç”Ÿæ•ˆ
                
                logger.debug("âš¡ æ‰§è¡ŒHMTä¼˜åŒ–æ¨ç†...")
                
            else:
                logger.debug("ğŸ“ ä½¿ç”¨æ ‡å‡†æ¨ç†è·¯å¾„...")
                generation_kwargs = {
                    "input_ids": request.input_tokens,
                    "max_new_tokens": request.max_tokens,
                    "temperature": request.temperature,
                    "top_p": request.top_p,
                    "top_k": request.top_k,
                    "do_sample": True
                }
            
            # æ‰§è¡Œç”Ÿæˆ
            generated_ids = self.simple_generator.generate(**generation_kwargs)
            
            # è§£ç è¾“å‡º
            input_length = request.input_tokens.shape[1]
            generated_tokens = generated_ids[0][input_length:]
            generated_text = self.model_loader.tokenizer.decode(
                generated_tokens, 
                skip_special_tokens=True
            )
            
            # ğŸ§  HMTåå¤„ç†ç»Ÿè®¡
            if self.hmt_manager and self.config.hmt.enable_hmt:
                self.hmt_manager.update_stats("cache_hits")
                
                # æ¯10æ¬¡ç”Ÿæˆè®°å½•ä¸€æ¬¡æ€§èƒ½æ‘˜è¦
                if self.hmt_manager.stats["total_allocations"] % 10 == 0:
                    logger.info("ğŸ“Š HMTæ€§èƒ½æ‘˜è¦:")
                    self.hmt_manager.log_performance_summary()
            
            return InferenceResponse(
                request_id=request.request_id,
                text=generated_text,
                finish_reason="stop", 
                tokens_generated=len(generated_tokens),
                model_info=self.model_loader.get_model_info()
            )
            
        except Exception as e:
            logger.error(f"å•æ¬¡æ¨ç†å¤±è´¥: {e}")
            raise
    

    
    async def _generate_streaming(self, request: InferenceRequest) -> InferenceResponse:
        """æµå¼æ¨ç†ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼Œç”¨äºç»Ÿä¸€æ¥å£ï¼‰"""
        try:
            # è¿™é‡Œå¯ä»¥å®ç°æµå¼æ¨ç†é€»è¾‘
            # æš‚æ—¶ä½¿ç”¨å•æ¬¡æ¨ç†æ¨¡æ‹Ÿ
            response = await self._generate_single(request)
            return response
            
        except Exception as e:
            logger.error(f"æµå¼æ¨ç†å¤±è´¥: {e}")
            raise
    
    async def _generate_streaming_async(self, request: InferenceRequest) -> AsyncGenerator[str, None]:
        """å¼‚æ­¥æµå¼æ¨ç†ç”Ÿæˆå™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        try:
            if not self.simple_generator:
                raise RuntimeError("ç®€åŒ–ç”Ÿæˆå™¨æœªåˆå§‹åŒ–")
            
            # ä½¿ç”¨ç®€åŒ–ç”Ÿæˆå™¨è¿›è¡Œæµå¼ç”Ÿæˆ
            # å½“å‰å®ç°ï¼šåˆ†å—ç”Ÿæˆæ–‡æœ¬
            chunk_size = 10  # æ¯æ¬¡ç”Ÿæˆ10ä¸ªtoken
            total_generated = 0
            
            while total_generated < request.max_tokens:
                remaining_tokens = min(chunk_size, request.max_tokens - total_generated)
                
                # ç”Ÿæˆä¸€å°å—æ–‡æœ¬
                chunk = self.simple_generator.generate_text(
                    text="",  # ç»­å†™å½“å‰å¯¹è¯
                    max_new_tokens=remaining_tokens,
                    temperature=request.temperature,
                    top_p=request.top_p
                )
                
                yield chunk
                total_generated += remaining_tokens
                    
        except Exception as e:
            logger.error(f"å¼‚æ­¥æµå¼æ¨ç†å¤±è´¥: {e}")
            yield f"[ERROR] {str(e)}"
    

    
    def _update_stats(self, success: bool, processing_time: float, tokens_generated: int):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        with self.stats_lock:
            if success:
                self.stats.successful_requests += 1
                self.stats.total_tokens_generated += tokens_generated
            else:
                self.stats.failed_requests += 1
            
            # æ›´æ–°å¹³å‡å»¶è¿Ÿ
            total_successful = self.stats.successful_requests
            if total_successful > 0:
                self.stats.average_latency = (
                    (self.stats.average_latency * (total_successful - 1) + processing_time) / total_successful
                )
            
            # æ›´æ–°token/ç§’
            elapsed_time = time.time() - self.stats.start_time
            if elapsed_time > 0:
                self.stats.tokens_per_second = self.stats.total_tokens_generated / elapsed_time
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ¨ç†ç»Ÿè®¡ä¿¡æ¯"""
        with self.stats_lock:
            stats = {
                "total_requests": self.stats.total_requests,
                "successful_requests": self.stats.successful_requests,
                "failed_requests": self.stats.failed_requests,
                "success_rate": (
                    self.stats.successful_requests / max(self.stats.total_requests, 1)
                ),
                "total_tokens_generated": self.stats.total_tokens_generated,
                "average_latency": self.stats.average_latency,
                "tokens_per_second": self.stats.tokens_per_second,
                "concurrent_requests": self.stats.concurrent_requests,
                "uptime": time.time() - self.stats.start_time,
                "model_info": self.model_loader.get_model_info() if self.model_loader else None,
            }
            
            # æ·»åŠ HMTç»Ÿè®¡ä¿¡æ¯
            hmt_stats = self.get_hmt_stats()
            if hmt_stats:
                stats["hmt_stats"] = hmt_stats
            
            # æ·»åŠ MoRç»Ÿè®¡ä¿¡æ¯
            mor_stats = self.get_mor_stats()
            if mor_stats:
                stats["mor_stats"] = mor_stats
            
            return stats
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥æ¨ç†å¼•æ“æ˜¯å¦å°±ç»ª"""
        return (
            self.is_initialized and 
            self.model_loader and 
            self.model_loader.is_model_loaded() and
                            self.simple_generator is not None
        )
    
    async def shutdown(self):
        """å…³é—­æ¨ç†å¼•æ“"""
        logger.info("ğŸ›‘ å…³é—­æ¨ç†å¼•æ“...")
        
        try:
            # å…³é—­çº¿ç¨‹æ± 
            self.executor.shutdown(wait=True)
            
            # æ¸…ç†ç®€åŒ–ç»„ä»¶
            if self.simple_forward:
                self.simple_forward.reset_stats()
                
            # æ¸…ç†ç®€åŒ–ç”Ÿæˆå™¨
            if self.simple_generator:
                self.simple_generator.reset_stats()
            
            # å¸è½½æ¨¡å‹
            if self.model_loader:
                self.model_loader.unload_model()
            
            # æ¸…ç†ç»„ä»¶å¼•ç”¨
            self.simple_forward = None
            self.simple_generator = None
            
            # æ¸…ç†å…¶ä»–èµ„æº
            self.is_initialized = False
            
            logger.info("æ¨ç†å¼•æ“å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"æ¨ç†å¼•æ“å…³é—­å¤±è´¥: {e}")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        await self.initialize()
        return self
    
    async def _generate_with_hmt_cache(self, model, request, generation_config, tokenizer):
        """ä½¿ç”¨HMTç¼“å­˜ä¼˜åŒ–çš„æ¨ç†"""
        try:
            logger.debug("ä½¿ç”¨HMTç¼“å­˜ä¼˜åŒ–æ¨ç†")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰SYMPHONYä¼šè¯ç¼“å­˜ï¼ˆä¸´æ—¶ç¦ç”¨ï¼‰
            # if hasattr(self.hmt_manager, 'symphony_manager') and self.hmt_manager.symphony_manager:
            #     # å°è¯•ä½¿ç”¨SYMPHONYå¤šè½®ä¼˜åŒ–
            #     symphony_result = await self.hmt_manager.symphony_manager.get_cached_response(
            #         request.input_text
            #     )
            #     if symphony_result:
            #         logger.debug("SYMPHONYç¼“å­˜å‘½ä¸­")
            #         # è¿™é‡Œåº”è¯¥è¿”å›ç¼“å­˜çš„ç»“æœï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç»§ç»­æ­£å¸¸æ¨ç†
            
            # ä½¿ç”¨ä¼˜åŒ–çš„ç”Ÿæˆé…ç½®
            if hasattr(self.hmt_manager, 'kv_cache_manager'):
                # é…ç½®KVç¼“å­˜ä¼˜åŒ–å‚æ•°
                generation_config.use_cache = True
                # ä¸´æ—¶ç¦ç”¨é‡åŒ–ç¼“å­˜ï¼Œä½¿ç”¨æ ‡å‡†ç¼“å­˜é¿å…layer_classeså‚æ•°é—®é¢˜
                # if hasattr(generation_config, 'cache_implementation'):
                #     generation_config.cache_implementation = "quantized"
            
            # å¦‚æœå¯ç”¨HeadInferï¼Œå¯ä»¥è¿›è¡Œå¤´çº§åˆ«offloading
            if hasattr(self.hmt_manager, 'head_offloader') and self.hmt_manager.head_offloader:
                logger.debug("ä½¿ç”¨HeadInferå¤´çº§åˆ«offloading")
                # è¿™é‡Œå¯ä»¥å®ç°å¤´çº§åˆ«çš„offloadingé€»è¾‘
                # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†ç”Ÿæˆ
            
            # å¦‚æœå¯ç”¨MoRè·¯ç”±ï¼Œä½¿ç”¨è·¯ç”±ä¼˜åŒ–çš„æ¨ç†
            if self.mor_router:
                outputs = await self._generate_with_mor_routing(
                    model, request, generation_config, tokenizer
                )
            else:
                # æ‰§è¡Œæ ‡å‡†æ¨ç†
                outputs = model.generate(
                    request.input_tokens,
                    generation_config=generation_config,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    do_sample=True
                )
            
            # æ›´æ–°HMTç»Ÿè®¡
            if hasattr(self.hmt_manager, 'stats'):
                self.hmt_manager.stats["cache_hits"] += 1
            
            return outputs
            
        except Exception as e:
            logger.error(f"HMTç¼“å­˜æ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†æ¨ç†
            return model.generate(
                request.input_tokens,
                generation_config=generation_config,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=True
            )
    
    def get_hmt_stats(self) -> Optional[Dict[str, Any]]:
        """è·å–HMTè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.hmt_manager:
            return {"hmt_enabled": False, "reason": "HMTç®¡ç†å™¨æœªåˆå§‹åŒ–"}
        
        try:
            # è·å–è¯¦ç»†çš„HMTç»Ÿè®¡ä¿¡æ¯
            detailed_stats = self.hmt_manager.get_hmt_detailed_stats()
            
            # æ·»åŠ é¢å¤–çš„æ¨ç†å¼•æ“ç»Ÿè®¡
            detailed_stats["inference_engine"] = {
                "successful_requests": self.stats.successful_requests,
                "failed_requests": self.stats.failed_requests,
                "average_latency": self.stats.average_latency,
                "total_tokens_generated": self.stats.total_tokens_generated,
                "tokens_per_second": self.stats.tokens_per_second
            }
            
            # æ·»åŠ æ¨¡å‹å†…å­˜ä¿¡æ¯
            if self.model_loader and hasattr(self.model_loader, 'get_model_info'):
                model_info = self.model_loader.get_model_info()
                detailed_stats["model_memory"] = {
                    "memory_usage_gb": model_info.get("memory_usage_gb", 0),
                    "num_parameters": model_info.get("num_parameters", 0),
                    "device": model_info.get("device", "unknown"),
                    "quantized": model_info.get("quantized", False)
                }
            
            return detailed_stats
            
        except Exception as e:
            logger.warning(f"è·å–HMTç»Ÿè®¡å¤±è´¥: {e}")
            return {"hmt_enabled": False, "error": str(e)}
    
    async def _generate_with_mor_routing(self, model, request, generation_config, tokenizer):
        """ä½¿ç”¨MoRè·¯ç”±ä¼˜åŒ–çš„æ¨ç†"""
        try:
            logger.debug("ä½¿ç”¨MoRåŠ¨æ€è·¯ç”±ä¼˜åŒ–æ¨ç†")
            
            # è·å–è¾“å…¥åµŒå…¥
            input_embeds = model.get_input_embeddings()(request.input_tokens)
            batch_size, seq_len, hidden_dim = input_embeds.shape
            
            # åˆ†æä»»åŠ¡ç±»å‹ï¼ˆåŸºäºè¾“å…¥å†…å®¹ï¼‰
            task_type = self._analyze_task_type(request.input_text)
            
            # åˆ›å»ºè·¯ç”±ä¸Šä¸‹æ–‡
            routing_context = {
                "task_type": task_type,
                "input_length": seq_len,
                "batch_size": batch_size,
                "temperature": request.temperature,
                "complexity_hint": self._estimate_complexity(request.input_text)
            }
            
            # ç”Ÿæˆæ—¶çš„è·¯ç”±ç»Ÿè®¡
            routing_stats = {
                "total_layers_routed": 0,
                "average_experts_per_layer": 0,
                "routing_decisions": []
            }
            
            # ä½¿ç”¨å¸¦è·¯ç”±ä¿¡æ¯çš„ç”Ÿæˆé…ç½®
            generation_config.output_hidden_states = True  # è·å–éšè—çŠ¶æ€ç”¨äºè·¯ç”±åˆ†æ
            generation_config.return_dict_in_generate = True
            
            # æ‰§è¡Œæ¨ç†ï¼ˆè¿™é‡Œæˆ‘ä»¬ä»ä½¿ç”¨æ ‡å‡†generateï¼Œä½†æ·»åŠ è·¯ç”±åˆ†æï¼‰
            with torch.no_grad():
                outputs = model.generate(
                    request.input_tokens,
                    generation_config=generation_config,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    do_sample=True
                )
                
                # å¦‚æœæœ‰éšè—çŠ¶æ€ï¼Œè¿›è¡Œè·¯ç”±åˆ†æ
                if hasattr(outputs, 'hidden_states') and outputs.hidden_states:
                    routing_analysis = await self._analyze_routing_efficiency(
                        outputs.hidden_states, routing_context
                    )
                    routing_stats.update(routing_analysis)
            
            # æ›´æ–°MoRç»Ÿè®¡
            if hasattr(self.mor_router, 'stats'):
                self.mor_router.stats["total_routing_decisions"] += routing_stats["total_layers_routed"]
            
            logger.debug(f"MoRè·¯ç”±ç»Ÿè®¡: {routing_stats}")
            
            # è¿”å›åºåˆ—ï¼ˆå…¼å®¹æ ‡å‡†generateè¾“å‡ºï¼‰
            if hasattr(outputs, 'sequences'):
                return outputs.sequences
            else:
                return outputs
                
        except Exception as e:
            logger.error(f"MoRè·¯ç”±æ¨ç†å¤±è´¥: {e}")
            # å›é€€åˆ°æ ‡å‡†æ¨ç†
            return model.generate(
                request.input_tokens,
                generation_config=generation_config,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=True
            )
    
    def _analyze_task_type(self, input_text: str) -> str:
        """åˆ†æä»»åŠ¡ç±»å‹"""
        text_lower = input_text.lower()
        
        # ç®€å•çš„ä»»åŠ¡ç±»å‹è¯†åˆ«
        if any(word in text_lower for word in ["ä»£ç ", "code", "ç¼–ç¨‹", "programming", "å‡½æ•°", "function"]):
            return "coding"
        elif any(word in text_lower for word in ["æ•°å­¦", "math", "è®¡ç®—", "solve", "æ–¹ç¨‹", "equation"]):
            return "mathematical"
        elif any(word in text_lower for word in ["ç¿»è¯‘", "translate", "translation"]):
            return "translation"
        elif any(word in text_lower for word in ["æ€»ç»“", "summary", "summarize", "æ‘˜è¦"]):
            return "summarization"
        elif any(word in text_lower for word in ["åˆ›ä½œ", "å†™ä½œ", "creative", "story", "å°è¯´"]):
            return "creative_writing"
        else:
            return "general_chat"
    
    def _estimate_complexity(self, input_text: str) -> float:
        """ä¼°ç®—ä»»åŠ¡å¤æ‚åº¦"""
        # åŸºäºæ–‡æœ¬é•¿åº¦ã€ç‰¹æ®Šå­—ç¬¦ç­‰ç®€å•ä¼°ç®—
        length_factor = min(len(input_text) / 1000.0, 1.0)  # é•¿åº¦å› å­
        
        # å¤æ‚å­—ç¬¦å› å­
        special_chars = sum(1 for c in input_text if not c.isalnum() and not c.isspace())
        special_factor = min(special_chars / 100.0, 1.0)
        
        # å¤šè¯­è¨€å› å­
        non_ascii = sum(1 for c in input_text if ord(c) > 127)
        multilingual_factor = min(non_ascii / len(input_text), 0.5) if input_text else 0
        
        complexity = (length_factor + special_factor + multilingual_factor) / 3.0
        return max(0.1, min(1.0, complexity))  # é™åˆ¶åœ¨0.1-1.0ä¹‹é—´
    
    async def _analyze_routing_efficiency(self, hidden_states, context):
        """åˆ†æè·¯ç”±æ•ˆç‡"""
        try:
            if not hidden_states or not self.mor_router:
                return {"routing_analysis": "not_available"}
            
            # ç®€åŒ–çš„è·¯ç”±æ•ˆç‡åˆ†æ
            num_layers = len(hidden_states)
            total_routing_decisions = 0
            expert_usage = {}
            
            for layer_idx, layer_hidden in enumerate(hidden_states):
                if layer_hidden is not None and layer_idx < num_layers - 1:  # è·³è¿‡æœ€åä¸€å±‚
                    # æ¨¡æ‹Ÿè·¯ç”±å†³ç­–
                    routing_weights, expert_indices, routing_info = self.mor_router.route(
                        layer_hidden,
                        layer_id=layer_idx,
                        task_type=context.get("task_type"),
                        context=context
                    )
                    
                    total_routing_decisions += 1
                    
                    # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨æƒ…å†µ
                    if expert_indices is not None:
                        for expert_id in expert_indices.flatten().tolist():
                            expert_usage[expert_id] = expert_usage.get(expert_id, 0) + 1
            
            avg_experts = len(expert_usage) / max(total_routing_decisions, 1)
            
            return {
                "total_layers_routed": total_routing_decisions,
                "average_experts_per_layer": avg_experts,
                "expert_usage_distribution": expert_usage,
                "routing_efficiency": total_routing_decisions / max(num_layers, 1)
            }
            
        except Exception as e:
            logger.warning(f"è·¯ç”±æ•ˆç‡åˆ†æå¤±è´¥: {e}")
            return {"routing_analysis": "failed", "error": str(e)}
    
    def get_mor_stats(self) -> Optional[Dict[str, Any]]:
        """è·å–MoRè·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
        if not self.mor_router:
            return None
        
        try:
            return {
                "mor_enabled": True,
                "routing_strategy": self.config.mor.routing_strategy,
                "num_experts": self.config.mor.num_experts,
                "router_stats": self.mor_router.stats,
                "expert_evolution_enabled": self.config.mor.enable_expert_evolution,
                "recursive_routing_enabled": self.config.mor.enable_recursive_routing,
                "dynamic_selection_enabled": self.config.mor.enable_dynamic_selection
            }
        except Exception as e:
            logger.warning(f"è·å–MoRç»Ÿè®¡å¤±è´¥: {e}")
            return {"mor_enabled": False, "error": str(e)}
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.shutdown()
        if exc_type:
            logger.error(f"InferenceEngineå¼‚æ­¥ä¸Šä¸‹æ–‡å¼‚å¸¸: {exc_type.__name__}: {exc_val}") 