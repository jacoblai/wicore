"""
ç®€åŒ–çš„æ¨¡å‹åŠ è½½å™¨

ä¸“æ³¨äºå•GPUéƒ¨ç½²ï¼Œæ”¯æŒï¼š
- è‡ªåŠ¨é‡åŒ–ä»¥é€‚åº”16GBå†…å­˜é™åˆ¶
- å¤šç§æ¨¡å‹æ¶æ„ï¼ˆQwenã€Llamaã€Gemmaã€Mistralç­‰ï¼‰
- é«˜æ•ˆçš„å†…å­˜ç®¡ç†
"""

import torch
import logging
from typing import Optional, Dict, Any, Union
from pathlib import Path
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    AutoConfig,
    BitsAndBytesConfig,
    GenerationConfig
)
from accelerate import init_empty_weights
import gc

logger = logging.getLogger(__name__)


class SimpleModelLoader:
    """ç®€åŒ–çš„æ¨¡å‹åŠ è½½å™¨
    
    ç‰¹ç‚¹ï¼š
    - å•GPUéƒ¨ç½²ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡
    - è‡ªåŠ¨é‡åŒ–æ”¯æŒï¼ˆINT4/INT8ï¼‰
    - å†…å­˜ä¼˜åŒ–åŠ è½½
    - æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„
    """
    
    def __init__(self, config):
        """åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨
        
        Args:
            config: é…ç½®å¯¹è±¡
        """
        self.config = config
        self.model = None
        self.tokenizer = None
        self.generation_config = None
        self.device = self._select_device()
        self.model_path = config.model_path
        self.enable_quantization = getattr(config, 'enable_quantization', True)
        
        # å†…å­˜ç›‘æ§
        self.memory_used = 0.0
        
        logger.info(f"ç®€åŒ–æ¨¡å‹åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ç›®æ ‡è®¾å¤‡: {self.device}")
        logger.info(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        logger.info(f"é‡åŒ–æ”¯æŒ: {self.enable_quantization}")
    
    def _select_device(self) -> torch.device:
        """é€‰æ‹©æœ€ä½³è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device('cuda:0')
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPUå†…å­˜: {gpu_memory:.1f}GB")
            return device
        else:
            logger.warning("æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨CPU")
            return torch.device('cpu')
    
    def _determine_quantization_config(self) -> Optional[BitsAndBytesConfig]:
        """ç¡®å®šé‡åŒ–é…ç½®"""
        if not self.enable_quantization or self.device.type == 'cpu':
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰bitsandbytes
        try:
            import bitsandbytes
        except ImportError:
            logger.warning("bitsandbytesæœªå®‰è£…ï¼Œç¦ç”¨é‡åŒ–ã€‚å»ºè®®: pip install bitsandbytes")
            return None
        
        # æ£€æŸ¥GPUå†…å­˜
        if torch.cuda.is_available():
            gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            if gpu_memory_gb <= 16:
                # 16GBåŠä»¥ä¸‹ï¼šä½¿ç”¨INT4é‡åŒ–
                logger.info("ä½¿ç”¨INT4é‡åŒ–ä»¥é€‚åº”16GBå†…å­˜é™åˆ¶")
                try:
                    return BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_use_double_quant=True,
                        bnb_4bit_quant_type="nf4",
                        bnb_4bit_compute_dtype=torch.float16
                    )
                except Exception as e:
                    logger.warning(f"é‡åŒ–é…ç½®å¤±è´¥ï¼Œä½¿ç”¨FP16: {e}")
                    return None
            elif gpu_memory_gb <= 24:
                # 24GBï¼šä½¿ç”¨INT8é‡åŒ–
                logger.info("ä½¿ç”¨INT8é‡åŒ–")
                try:
                    return BitsAndBytesConfig(
                        load_in_8bit=True
                    )
                except Exception as e:
                    logger.warning(f"é‡åŒ–é…ç½®å¤±è´¥ï¼Œä½¿ç”¨FP16: {e}")
                    return None
            else:
                # 24GBä»¥ä¸Šï¼šä¸ä½¿ç”¨é‡åŒ–
                logger.info("GPUå†…å­˜å……è¶³ï¼Œä¸ä½¿ç”¨é‡åŒ–")
                return None
        
        return None
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {self.model_path}")
        
        try:
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                gc.collect()
            
            # åŠ è½½é…ç½®
            self.model_config = AutoConfig.from_pretrained(self.model_path, trust_remote_code=True)
            logger.info(f"æ¨¡å‹æ¶æ„: {self.model_config.model_type}")
            logger.info(f"è¯æ±‡è¡¨å¤§å°: {self.model_config.vocab_size}")
            
            # ç¡®å®šé‡åŒ–é…ç½®
            quantization_config = self._determine_quantization_config()
            
            # åŠ è½½åˆ†è¯å™¨
            logger.info("åŠ è½½åˆ†è¯å™¨...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                use_fast=True
            )
            
            # è®¾ç½®padding token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½æ¨¡å‹
            logger.info("åŠ è½½æ¨¡å‹...")
            model_kwargs = {
                'pretrained_model_name_or_path': self.model_path,
                'config': self.model_config,
                'torch_dtype': torch.float16 if self.device.type == 'cuda' else torch.float32,
                'device_map': self.device,
                'trust_remote_code': True,
                'low_cpu_mem_usage': True,
            }
            
            if quantization_config is not None:
                model_kwargs['quantization_config'] = quantization_config
                # é‡åŒ–æ¨¡å‹ä¸èƒ½æŒ‡å®šdevice_map
                del model_kwargs['device_map']
            
            self.model = AutoModelForCausalLM.from_pretrained(**model_kwargs)
            
            # å¦‚æœæ²¡æœ‰ä½¿ç”¨é‡åŒ–ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
            if quantization_config is None:
                self.model = self.model.to(self.device)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            
            # è®¾ç½®ç”Ÿæˆé…ç½®
            self._setup_generation_config()
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨
            self._calculate_memory_usage()
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            logger.info(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
            logger.info(f"å†…å­˜ä½¿ç”¨: {self.memory_used:.2f}GB")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_generation_config(self):
        """è®¾ç½®ç”Ÿæˆé…ç½®"""
        try:
            self.generation_config = GenerationConfig.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
        except:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç”Ÿæˆé…ç½®ï¼Œåˆ›å»ºé»˜è®¤é…ç½®
            self.generation_config = GenerationConfig(
                max_length=2048,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        logger.info("ç”Ÿæˆé…ç½®è®¾ç½®å®Œæˆ")
    
    def _calculate_memory_usage(self):
        """è®¡ç®—å†…å­˜ä½¿ç”¨é‡"""
        if self.device.type == 'cuda':
            # GPUå†…å­˜
            self.memory_used = torch.cuda.memory_allocated() / 1024**3
        else:
            # ä¼°ç®—CPUå†…å­˜
            param_size = sum(p.numel() * p.element_size() for p in self.model.parameters())
            self.memory_used = param_size / 1024**3
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return {"status": "not_loaded"}
        
        return {
            "model_type": self.model_config.model_type,
            "vocab_size": self.model_config.vocab_size,
            "hidden_size": getattr(self.model_config, 'hidden_size', 'unknown'),
            "num_layers": getattr(self.model_config, 'num_hidden_layers', 'unknown'),
            "num_parameters": sum(p.numel() for p in self.model.parameters()),
            "memory_usage_gb": self.memory_used,
            "device": str(self.device),
            "quantized": hasattr(self.model, 'quantization_config')
        }
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹"""
        if self.model is not None:
            logger.info("ğŸ—‘ï¸ å¸è½½æ¨¡å‹...")
            del self.model
            self.model = None
        
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        
        if self.generation_config is not None:
            del self.generation_config
            self.generation_config = None
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.memory_used = 0.0
        logger.info("æ¨¡å‹å¸è½½å®Œæˆ")
    
    @property
    def is_loaded(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        return self.model is not None and self.tokenizer is not None 