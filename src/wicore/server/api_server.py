"""
WiCore APIæœåŠ¡å™¨
åŸºäºFastAPIå®ç°OpenAIå…¼å®¹çš„æ¨ç†æ¥å£

æ ¸å¿ƒåŠŸèƒ½:
- OpenAIå…¼å®¹çš„APIæ¥å£
- æµå¼å’Œéæµå¼æ–‡æœ¬ç”Ÿæˆ
- å¥åº·æ£€æŸ¥å’ŒçŠ¶æ€ç›‘æ§
- è¯·æ±‚é™æµå’Œè´Ÿè½½å‡è¡¡
- å®Œæ•´çš„é”™è¯¯å¤„ç†
"""

import asyncio
import logging
import time
import uuid
from typing import Dict, List, Optional, Any, AsyncGenerator
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import uvicorn

from ..core.inference_engine import InferenceEngine, InferenceRequest, InferenceResponse
from ..core.config import WiCoreConfig, get_config_manager

logger = logging.getLogger(__name__)


# ==================== è¯·æ±‚/å“åº”æ¨¡å‹ ====================

class ChatMessage(BaseModel):
    """èŠå¤©æ¶ˆæ¯"""
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: system, user, assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")


class ChatCompletionRequest(BaseModel):
    """èŠå¤©å®Œæˆè¯·æ±‚"""
    model: str = Field(default="gemma-3-27b-it", description="æ¨¡å‹åç§°")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    max_tokens: Optional[int] = Field(default=512, ge=1, le=8192, description="æœ€å¤§ç”Ÿæˆtokenæ•°")
    temperature: Optional[float] = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    top_p: Optional[float] = Field(default=0.9, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·å‚æ•°")
    top_k: Optional[int] = Field(default=50, ge=1, le=100, description="Top-Ké‡‡æ ·")
    stream: Optional[bool] = Field(default=False, description="æ˜¯å¦æµå¼è¿”å›")
    stop: Optional[List[str]] = Field(default=None, description="åœæ­¢åºåˆ—")


class ChatCompletionResponseChoice(BaseModel):
    """èŠå¤©å®Œæˆå“åº”é€‰æ‹©"""
    index: int
    message: ChatMessage
    finish_reason: str  # stop, length, error


class ChatCompletionResponse(BaseModel):
    """èŠå¤©å®Œæˆå“åº”"""
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionResponseChoice]
    usage: Dict[str, int]


class ChatCompletionStreamChunk(BaseModel):
    """æµå¼èŠå¤©å®Œæˆå“åº”å—"""
    id: str
    object: str = "chat.completion.chunk"
    created: int
    model: str
    choices: List[Dict[str, Any]]


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    timestamp: float
    version: str
    uptime: float


class StatusResponse(BaseModel):
    """ç³»ç»ŸçŠ¶æ€å“åº”"""
    engine_ready: bool
    model_loaded: bool
    stats: Dict[str, Any]
    config: Dict[str, Any]


# ==================== å…¨å±€çŠ¶æ€ ====================

class APIServerState:
    """APIæœåŠ¡å™¨çŠ¶æ€"""
    
    def __init__(self):
        self.inference_engine: Optional[InferenceEngine] = None
        self.config: Optional[WiCoreConfig] = None
        self.start_time = time.time()
        self.request_count = 0


# å…¨å±€çŠ¶æ€å®ä¾‹
api_state = APIServerState()


# ==================== ä¾èµ–æ³¨å…¥ ====================

async def get_inference_engine() -> InferenceEngine:
    """è·å–æ¨ç†å¼•æ“ä¾èµ–"""
    if not api_state.inference_engine or not api_state.inference_engine.is_ready():
        raise HTTPException(
            status_code=503,
            detail="æ¨ç†å¼•æ“æœªå°±ç»ªï¼Œè¯·ç¨åé‡è¯•"
        )
    return api_state.inference_engine


# ==================== ç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("ğŸš€ å¯åŠ¨WiCore APIæœåŠ¡å™¨...")
    
    try:
        # åŠ è½½é…ç½®
        config_manager = get_config_manager()
        api_state.config = config_manager.get_config()
        
        if not api_state.config:
            # å¦‚æœæ²¡æœ‰é…ç½®ï¼ŒåŠ è½½é»˜è®¤é…ç½®
            api_state.config = config_manager.load_config(environment="development")
        
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        api_state.inference_engine = InferenceEngine(api_state.config)
        
        # å¯åŠ¨æ¨ç†å¼•æ“
        success = await api_state.inference_engine.initialize()
        if not success:
            raise RuntimeError("æ¨ç†å¼•æ“åˆå§‹åŒ–å¤±è´¥")
        
        logger.info("âœ… WiCore APIæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
        
        yield  # æœåŠ¡å™¨è¿è¡ŒæœŸé—´
        
    except Exception as e:
        logger.error(f"âŒ APIæœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        raise
    
    finally:
        # å…³é—­æ—¶æ¸…ç†
        logger.info("ğŸ›‘ å…³é—­WiCore APIæœåŠ¡å™¨...")
        
        if api_state.inference_engine:
            await api_state.inference_engine.shutdown()
        
        logger.info("ğŸ‘‹ WiCore APIæœåŠ¡å™¨å·²å…³é—­")


# ==================== FastAPIåº”ç”¨ ====================

app = FastAPI(
    title="WiCore API",
    description="ä¸–ç•Œçº§é«˜æ€§èƒ½LLMæ¨ç†å¼•æ“ - OpenAIå…¼å®¹API",
    version="1.0.0",
    lifespan=lifespan
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== ä¸­é—´ä»¶ ====================

@app.middleware("http")
async def request_middleware(request: Request, call_next):
    """è¯·æ±‚ä¸­é—´ä»¶"""
    start_time = time.time()
    api_state.request_count += 1
    
    # æ·»åŠ è¯·æ±‚ID
    request_id = str(uuid.uuid4())
    request.state.request_id = request_id
    
    # è®°å½•è¯·æ±‚
    logger.info(f"æ”¶åˆ°è¯·æ±‚ {request_id}: {request.method} {request.url}")
    
    try:
        response = await call_next(request)
        
        # è®°å½•å“åº”æ—¶é—´
        process_time = time.time() - start_time
        response.headers["X-Request-ID"] = request_id
        response.headers["X-Process-Time"] = str(process_time)
        
        logger.info(f"è¯·æ±‚å®Œæˆ {request_id}: {response.status_code} ({process_time:.3f}s)")
        
        return response
        
    except Exception as e:
        process_time = time.time() - start_time
        logger.error(f"è¯·æ±‚å¤±è´¥ {request_id}: {e} ({process_time:.3f}s)")
        raise


# ==================== APIè·¯ç”± ====================

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return HealthResponse(
        status="healthy",
        timestamp=time.time(),
        version="1.0.0",
        uptime=time.time() - api_state.start_time
    )


@app.get("/status", response_model=StatusResponse)
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    engine_ready = (
        api_state.inference_engine is not None and 
        api_state.inference_engine.is_ready()
    )
    
    model_loaded = False
    stats = {}
    config_info = {}
    
    if api_state.inference_engine:
        model_loaded = (
            api_state.inference_engine.model_loader and
            api_state.inference_engine.model_loader.is_model_loaded()
        )
        stats = api_state.inference_engine.get_stats()
    
    if api_state.config:
        config_info = {
            "model_name": api_state.config.model.model_name,
            "model_type": api_state.config.model.model_type,
            "max_batch_size": api_state.config.model.max_batch_size,
            "hmt_enabled": api_state.config.hmt.enable_hmt,
            "mor_enabled": api_state.config.mor.enable_mor,
        }
    
    return StatusResponse(
        engine_ready=engine_ready,
        model_loaded=model_loaded,
        stats=stats,
        config=config_info
    )


@app.post("/v1/chat/completions")
async def chat_completions(
    request: ChatCompletionRequest,
    engine: InferenceEngine = Depends(get_inference_engine)
):
    """OpenAIå…¼å®¹çš„èŠå¤©å®Œæˆæ¥å£"""
    request_id = str(uuid.uuid4())
    
    try:
        # æ„å»ºæ¨ç†è¯·æ±‚
        inference_request = InferenceRequest(
            request_id=request_id,
            messages=[{"role": msg.role, "content": msg.content} for msg in request.messages],
            max_tokens=request.max_tokens or 512,
            temperature=request.temperature or 0.7,
            top_p=request.top_p or 0.9,
            top_k=request.top_k or 50,
            stream=request.stream or False,
            stop_sequences=request.stop
        )
        
        if request.stream:
            # æµå¼å“åº”
            return StreamingResponse(
                stream_chat_completions(engine, inference_request, request.model),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Request-ID": request_id
                }
            )
        else:
            # éæµå¼å“åº”
            response = await engine.generate_text(inference_request)
            
            return ChatCompletionResponse(
                id=request_id,
                created=int(time.time()),
                model=request.model,
                choices=[
                    ChatCompletionResponseChoice(
                        index=0,
                        message=ChatMessage(role="assistant", content=response.text),
                        finish_reason=response.finish_reason
                    )
                ],
                usage={
                    "prompt_tokens": len(inference_request.input_text.split()) if inference_request.input_text else 0,
                    "completion_tokens": response.tokens_generated,
                    "total_tokens": len(inference_request.input_text.split()) + response.tokens_generated
                }
            )
            
    except Exception as e:
        logger.error(f"èŠå¤©å®Œæˆè¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¨ç†å¤±è´¥: {str(e)}")


async def stream_chat_completions(
    engine: InferenceEngine,
    inference_request: InferenceRequest, 
    model_name: str
) -> AsyncGenerator[str, None]:
    """æµå¼èŠå¤©å®Œæˆç”Ÿæˆå™¨"""
    request_id = inference_request.request_id
    created = int(time.time())
    
    try:
        # å‘é€å¼€å§‹äº‹ä»¶
        start_chunk = ChatCompletionStreamChunk(
            id=request_id,
            created=created,
            model=model_name,
            choices=[{
                "index": 0,
                "delta": {"role": "assistant", "content": ""},
                "finish_reason": None
            }]
        )
        yield f"data: {start_chunk.model_dump_json()}\n\n"
        
        # æµå¼ç”Ÿæˆæ–‡æœ¬
        async for chunk in engine.generate_stream(inference_request):
            if chunk.strip():  # å¿½ç•¥ç©ºå—
                stream_chunk = ChatCompletionStreamChunk(
                    id=request_id,
                    created=created,
                    model=model_name,
                    choices=[{
                        "index": 0,
                        "delta": {"content": chunk},
                        "finish_reason": None
                    }]
                )
                yield f"data: {stream_chunk.model_dump_json()}\n\n"
        
        # å‘é€ç»“æŸäº‹ä»¶
        end_chunk = ChatCompletionStreamChunk(
            id=request_id,
            created=created,
            model=model_name,
            choices=[{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        )
        yield f"data: {end_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"
        
    except Exception as e:
        logger.error(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
        error_chunk = ChatCompletionStreamChunk(
            id=request_id,
            created=created,
            model=model_name,
            choices=[{
                "index": 0,
                "delta": {"content": f"[ERROR] {str(e)}"},
                "finish_reason": "error"
            }]
        )
        yield f"data: {error_chunk.model_dump_json()}\n\n"
        yield "data: [DONE]\n\n"


@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    models = []
    
    if api_state.config and api_state.config.model.model_name:
        models.append({
            "id": api_state.config.model.model_name,
            "object": "model",
            "created": int(api_state.start_time),
            "owned_by": "wicore"
        })
    
    return {"object": "list", "data": models}


# ==================== é”™è¯¯å¤„ç† ====================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTPå¼‚å¸¸å¤„ç†"""
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": {
                "message": exc.detail,
                "type": "http_error",
                "code": exc.status_code
            }
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """é€šç”¨å¼‚å¸¸å¤„ç†"""
    logger.error(f"æœªå¤„ç†å¼‚å¸¸: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": {
                "message": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯",
                "type": "internal_error",
                "code": 500
            }
        }
    )


# ==================== æœåŠ¡å™¨å¯åŠ¨å‡½æ•° ====================

def create_app(config: Optional[WiCoreConfig] = None) -> FastAPI:
    """åˆ›å»ºFastAPIåº”ç”¨"""
    if config:
        api_state.config = config
    return app


async def start_server(
    config: Optional[WiCoreConfig] = None,
    host: str = "0.0.0.0",
    port: int = 8000,
    workers: int = 1
):
    """å¯åŠ¨APIæœåŠ¡å™¨"""
    if config:
        api_state.config = config
        host = config.server.host
        port = config.server.port
        workers = config.server.workers
    
    # é…ç½®uvicorn
    uvicorn_config = uvicorn.Config(
        app=app,
        host=host,
        port=port,
        workers=workers,
        log_level="info",
        access_log=True
    )
    
    server = uvicorn.Server(uvicorn_config)
    
    logger.info(f"ğŸŒ å¯åŠ¨APIæœåŠ¡å™¨: http://{host}:{port}")
    await server.serve()


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ—¶çš„å…¥å£ç‚¹
    asyncio.run(start_server()) 