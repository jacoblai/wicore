"""
HMT (Hierarchical Memory Tiering) ç®¡ç†å™¨
æ•´åˆ2024-2025æœ€æ–°å†…å­˜ä¼˜åŒ–æŠ€æœ¯çš„æ ¸å¿ƒç®¡ç†ç³»ç»Ÿ

åŸºäºä»¥ä¸‹çªç ´æ€§ç ”ç©¶:
- Jenga: å¼‚æ„åµŒå…¥å†…å­˜åˆ†é… (ArXiv 2503.18292)
- HeadInfer: å¤´çº§åˆ«KVç¼“å­˜offloading (ArXiv 2502.12574) 
- vTensor: GPUè™šæ‹Ÿå†…å­˜ç®¡ç† (ArXiv 2407.15309)
- MiniKV: 2ä½é‡åŒ–KVç¼“å­˜ (ArXiv 2411.18077)
- LaCache: é˜¶æ¢¯å½¢ç¼“å­˜ç»“æ„ (ArXiv 2507.14204)
- SYMPHONY: å¤šè½®äº¤äº’ä¼˜åŒ– (ArXiv 2412.16434)
"""

import torch
import logging
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor

from ..core.config import HMTConfig
from .vtensor import VTensorManager
from .jenga_allocator import JengaAllocator  
from .kv_cache import KVCacheManager, MiniKVCache, LaCacheManager
from .head_infer import HeadInferOffloader
from .symphony import SymphonyManager

logger = logging.getLogger(__name__)


class MemoryTier(Enum):
    """å†…å­˜å±‚çº§å®šä¹‰"""
    GPU_HBM = "gpu_hbm"        # GPUé«˜å¸¦å®½å†…å­˜ (æœ€å¿«)
    GPU_SHARED = "gpu_shared"   # GPUå…±äº«å†…å­˜
    CPU_DRAM = "cpu_dram"      # CPUå†…å­˜ (ä¸­ç­‰)  
    NVME_SSD = "nvme_ssd"      # NVMeå­˜å‚¨ (æœ€æ…¢ä½†å®¹é‡å¤§)


@dataclass
class HMTConfig:
    """HMTé…ç½®å‚æ•°"""
    # GPUå†…å­˜é…ç½®
    gpu_memory_pool_size: int = 32 * 1024**3  # 32GB
    gpu_reserved_memory: int = 4 * 1024**3    # 4GBé¢„ç•™
    
    # åˆ†å±‚å†…å­˜é…ç½®
    tier_ratios: Dict[MemoryTier, float] = None  # å„å±‚å†…å­˜æ¯”ä¾‹
    tier_bandwidth: Dict[MemoryTier, float] = None  # å„å±‚å¸¦å®½ GB/s
    
    # KVç¼“å­˜ä¼˜åŒ–é…ç½®
    enable_minikv: bool = True          # å¯ç”¨2ä½é‡åŒ–
    minikv_compression_ratio: float = 0.86  # å‹ç¼©æ¯”ä¾‹
    enable_lacache: bool = True         # å¯ç”¨é˜¶æ¢¯å½¢ç¼“å­˜
    lacache_levels: int = 4             # ç¼“å­˜å±‚çº§æ•°
    
    # HeadInferé…ç½®
    enable_head_offload: bool = True    # å¯ç”¨å¤´çº§åˆ«offloading
    head_offload_ratio: float = 0.8     # offloadæ¯”ä¾‹
    
    # SYMPHONYé…ç½®  
    enable_symphony: bool = True        # å¯ç”¨å¤šè½®ä¼˜åŒ–
    symphony_window_size: int = 128     # ä¼šè¯çª—å£å¤§å°
    
    # æ€§èƒ½è°ƒä¼˜
    prefetch_enabled: bool = True       # å¯ç”¨é¢„å–
    async_offload: bool = True          # å¼‚æ­¥offloading
    memory_pool_threads: int = 4        # å†…å­˜æ± çº¿ç¨‹æ•°

    def __post_init__(self):
        """åˆå§‹åŒ–é»˜è®¤é…ç½®"""
        if self.tier_ratios is None:
            self.tier_ratios = {
                MemoryTier.GPU_HBM: 0.6,      # 60% GPU HBM
                MemoryTier.GPU_SHARED: 0.2,   # 20% GPUå…±äº«
                MemoryTier.CPU_DRAM: 0.15,    # 15% CPUå†…å­˜
                MemoryTier.NVME_SSD: 0.05,    # 5% NVMeå­˜å‚¨
            }
        
        if self.tier_bandwidth is None:
            self.tier_bandwidth = {
                MemoryTier.GPU_HBM: 2000.0,    # 2TB/s (HBM3)
                MemoryTier.GPU_SHARED: 1000.0,  # 1TB/s 
                MemoryTier.CPU_DRAM: 100.0,     # 100GB/s (DDR5)
                MemoryTier.NVME_SSD: 10.0,      # 10GB/s (PCIe 5.0)
            }


class HMTManager:
    """
    HMTå†…å­˜ç®¡ç†å™¨ - åè°ƒæ‰€æœ‰å†…å­˜ä¼˜åŒ–æŠ€æœ¯
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. åˆ†å±‚å†…å­˜ç®¡ç† (GPUâ†’CPUâ†’NVMe)
    2. æ™ºèƒ½KVç¼“å­˜ä¼˜åŒ– (MiniKV + LaCache)
    3. åŠ¨æ€å¤´çº§åˆ«offloading (HeadInfer)
    4. å¤šè½®äº¤äº’ä¼˜åŒ– (SYMPHONY)
    5. è™šæ‹Ÿå†…å­˜ç®¡ç† (vTensor)
    6. å¼‚æ„å†…å­˜åˆ†é… (Jenga)
    """
    
    def __init__(self, config: HMTConfig):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        logger.info("ğŸš€ åˆå§‹åŒ– HMT (Hierarchical Memory Tiering) ç³»ç»Ÿ...")
        logger.info("ğŸ“‹ HMT éªŒè¯ç›®æ ‡ï¼šæ”¯æŒåƒäº¿æ¨¡å‹å•å¡éƒ¨ç½²ä¸128Kä¸Šä¸‹æ–‡")
        
        # åˆå§‹åŒ–å„ä¸ªå­ç³»ç»Ÿ
        self._init_subsystems()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_allocations": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "offload_operations": 0,
            "memory_usage": {},
            "vtensor_operations": 0,
            "jenga_allocations": 0,
            "lacache_hits": 0,
            "symphony_cache_hits": 0,
            "head_offload_saves_mb": 0,
        }
        
        # è¾“å‡ºç³»ç»Ÿæ¦‚å†µ
        self._log_system_overview()
        
        logger.info("âœ… HMT Manager åˆå§‹åŒ–å®Œæˆ - é›†æˆ2024-2025æœ€æ–°å†…å­˜ä¼˜åŒ–æŠ€æœ¯")
    
    def _init_subsystems(self):
        """åˆå§‹åŒ–å„ä¸ªå­ç³»ç»Ÿ"""
        logger.info("ğŸ”§ åˆå§‹åŒ– HMT æ ¸å¿ƒå­ç³»ç»Ÿ...")
        
        # ğŸ“¦ 1. vTensorè™šæ‹Ÿå†…å­˜ç®¡ç†å™¨ (ArXiv 2407.15309)
        if getattr(self.config, 'enable_vtensor', True):
            logger.info("ğŸ“¦ åˆå§‹åŒ– vTensor è™šæ‹Ÿå†…å­˜ç®¡ç†å™¨...")
            try:
                from .vtensor import VTensorConfig
                vtensor_config = VTensorConfig()
                self.vtensor_manager = VTensorManager(vtensor_config)
                logger.info(f"âœ… vTensor å¯åŠ¨æˆåŠŸ - é¡µé¢å¤§å°: {getattr(self.config, 'vtensor_page_size_mb', 64)}MB")
            except Exception as e:
                logger.warning(f"âš ï¸  vTensor åˆå§‹åŒ–å¤±è´¥: {e}")
                self.vtensor_manager = None
        else:
            logger.info("â­ï¸  vTensor å·²ç¦ç”¨")
            self.vtensor_manager = None
        
        # ğŸ§© 2. Jengaå¼‚æ„å†…å­˜åˆ†é…å™¨ (ArXiv 2503.18292)
        if getattr(self.config, 'enable_jenga', True):
            logger.info("ğŸ§© åˆå§‹åŒ– Jenga å¼‚æ„åµŒå…¥å†…å­˜åˆ†é…å™¨...")
            try:
                from .jenga_allocator import JengaConfig
                jenga_config = JengaConfig()
                self.jenga_allocator = JengaAllocator(jenga_config)
                gpu_ratio = getattr(self.config, 'jenga_gpu_embedding_ratio', 0.7)
                logger.info(f"âœ… Jenga å¯åŠ¨æˆåŠŸ - GPUåµŒå…¥æ¯”ä¾‹: {gpu_ratio*100:.1f}%")
            except Exception as e:
                logger.warning(f"âš ï¸  Jenga åˆå§‹åŒ–å¤±è´¥: {e}")
                self.jenga_allocator = None
        else:
            logger.info("â­ï¸  Jenga å·²ç¦ç”¨")
            self.jenga_allocator = None
        
        # ğŸ’¾ 3. KVç¼“å­˜ç®¡ç†å™¨ (MiniKV + LaCache)
        logger.info("ğŸ’¾ åˆå§‹åŒ– KVç¼“å­˜ç®¡ç†å™¨...")
        try:
            from .kv_cache import KVCacheConfig
            kv_config = KVCacheConfig(
                enable_quantization=getattr(self.config, 'enable_minikv', False),
                quantization_bits=getattr(self.config, 'minikv_quantization_bits', 2),
                enable_ladder_cache=getattr(self.config, 'enable_lacache', False),
                ladder_levels=getattr(self.config, 'lacache_levels', 3),
                enable_symphony=getattr(self.config, 'enable_symphony', False)
            )
            self.kv_cache_manager = KVCacheManager(kv_config)
            
            # è®°å½•å¯ç”¨çš„ç‰¹æ€§
            features = []
            if kv_config.enable_quantization:
                features.append(f"MiniKV({kv_config.quantization_bits}bit)")
            if kv_config.enable_ladder_cache:
                features.append(f"LaCache({kv_config.ladder_levels}å±‚)")
            if kv_config.enable_symphony:
                features.append("SYMPHONY")
            
            logger.info(f"âœ… KVç¼“å­˜ç®¡ç†å™¨å¯åŠ¨æˆåŠŸ - ç‰¹æ€§: {', '.join(features) if features else 'æ ‡å‡†ç¼“å­˜'}")
        except Exception as e:
            logger.warning(f"âš ï¸  KVç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.kv_cache_manager = None
        
        # ğŸ¯ 4. HeadInfer offloader (ArXiv 2502.12574)
        if getattr(self.config, 'enable_head_offload', False):
            logger.info("ğŸ¯ åˆå§‹åŒ– HeadInfer å¤´çº§åˆ«offloading...")
            try:
                self.head_offloader = HeadInferOffloader(
                    num_heads=32,  # é»˜è®¤å€¼ï¼Œåç»­ä¼šä»æ¨¡å‹é…ç½®è·å–
                    offload_ratio=getattr(self.config, 'head_offload_ratio', 0.3)
                )
                offload_ratio = getattr(self.config, 'head_offload_ratio', 0.3)
                logger.info(f"âœ… HeadInfer å¯åŠ¨æˆåŠŸ - offloadæ¯”ä¾‹: {offload_ratio*100:.1f}%")
            except Exception as e:
                logger.warning(f"âš ï¸  HeadInfer åˆå§‹åŒ–å¤±è´¥: {e}")
                self.head_offloader = None
        else:
            logger.info("â­ï¸  HeadInfer å·²ç¦ç”¨")
            self.head_offloader = None
        
        # ğŸµ 5. SYMPHONYå¤šè½®ä¼˜åŒ–å™¨ (ArXiv 2412.16434)
        if getattr(self.config, 'enable_symphony', False):
            logger.info("ğŸµ åˆå§‹åŒ– SYMPHONY å¤šè½®äº¤äº’ä¼˜åŒ–å™¨...")
            try:
                self.symphony_manager = SymphonyManager(
                    max_concurrent_requests=getattr(self.config, 'symphony_window_size', 8)
                )
                window_size = getattr(self.config, 'symphony_window_size', 8)
                logger.info(f"âœ… SYMPHONY å¯åŠ¨æˆåŠŸ - çª—å£å¤§å°: {window_size}")
            except Exception as e:
                logger.warning(f"âš ï¸  SYMPHONY åˆå§‹åŒ–å¤±è´¥: {e}")
                self.symphony_manager = None
        else:
            logger.info("â­ï¸  SYMPHONY å·²ç¦ç”¨")
            self.symphony_manager = None
        
        # ğŸ”§ 6. çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥æ“ä½œ
        max_workers = getattr(self.config, 'memory_pool_threads', 4)
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)
        logger.info(f"ğŸ”§ å¼‚æ­¥æ“ä½œçº¿ç¨‹æ± å¯åŠ¨ - å·¥ä½œçº¿ç¨‹: {max_workers}")
        
        logger.info("âœ… HMT å­ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _log_system_overview(self):
        """è¾“å‡ºHMTç³»ç»Ÿæ¦‚å†µ"""
        logger.info("ğŸ“Š HMTç³»ç»Ÿæ¦‚å†µ:")
        
        # ğŸ§  åˆ†å±‚å†…å­˜é…ç½®
        gpu_max = getattr(self.config, 'memory_pools', {}).get('gpu', {}).get('max_size_gb', 0)
        cpu_max = getattr(self.config, 'memory_pools', {}).get('cpu', {}).get('max_size_gb', 0)
        nvme_max = getattr(self.config, 'memory_pools', {}).get('nvme', {}).get('max_size_gb', 0)
        logger.info(f"ğŸ§  åˆ†å±‚å†…å­˜: GPU({gpu_max}GB) â†’ CPU({cpu_max}GB) â†’ NVMe({nvme_max}GB)")
        
        # ğŸ“‹ å¯ç”¨çš„æŠ€æœ¯
        enabled_techs = []
        if getattr(self.config, 'enable_minikv', False):
            bits = getattr(self.config, 'minikv_quantization_bits', 2)
            enabled_techs.append(f"MiniKV({bits}bit)")
        if getattr(self.config, 'enable_lacache', False):
            levels = getattr(self.config, 'lacache_levels', 3)
            enabled_techs.append(f"LaCache({levels}å±‚)")
        if getattr(self.config, 'enable_head_offload', False):
            ratio = getattr(self.config, 'head_offload_ratio', 0.3)
            enabled_techs.append(f"HeadInfer({ratio*100:.0f}%)")
        if getattr(self.config, 'enable_symphony', False):
            window = getattr(self.config, 'symphony_window_size', 8)
            enabled_techs.append(f"SYMPHONY({window})")
        if getattr(self.config, 'enable_vtensor', False):
            page_size = getattr(self.config, 'vtensor_page_size_mb', 64)
            enabled_techs.append(f"vTensor({page_size}MB)")
        if getattr(self.config, 'enable_jenga', False):
            gpu_ratio = getattr(self.config, 'jenga_gpu_embedding_ratio', 0.7)
            enabled_techs.append(f"Jenga({gpu_ratio*100:.0f}%)")
        
        logger.info(f"ğŸ”¬ å¯ç”¨æŠ€æœ¯: {', '.join(enabled_techs) if enabled_techs else 'åŸºç¡€å†…å­˜ç®¡ç†'}")
        
        # ğŸ¯ éªŒè¯ç›®æ ‡
        logger.info("ğŸ¯ éªŒè¯ç›®æ ‡:")
        logger.info("   âœ“ åƒäº¿æ¨¡å‹å•å¡éƒ¨ç½²")
        logger.info("   âœ“ 128Kä¸Šä¸‹æ–‡æ”¯æŒ")
        logger.info("   âœ“ GPUâ†’CPUâ†’NVMeåˆ†å±‚å­˜å‚¨")
        logger.info("   âœ“ æ™ºèƒ½ç¼“å­˜ä¼˜åŒ–")
        logger.info("   âœ“ åŠ¨æ€å†…å­˜ç®¡ç†")
    
    def get_hmt_detailed_stats(self) -> Dict[str, Any]:
        """è·å–HMTè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        detailed_stats = {
            "hmt_enabled": True,
            "subsystems": {},
            "performance": self.stats.copy(),
            "memory_breakdown": {}
        }
        
        # vTensorç»Ÿè®¡
        if self.vtensor_manager:
            detailed_stats["subsystems"]["vtensor"] = {
                "enabled": True,
                "page_size_mb": getattr(self.config, 'vtensor_page_size_mb', 64),
                "operations": self.stats.get("vtensor_operations", 0),
                "status": "è¿è¡Œä¸­"
            }
        else:
            detailed_stats["subsystems"]["vtensor"] = {"enabled": False}
        
        # Jengaç»Ÿè®¡
        if self.jenga_allocator:
            detailed_stats["subsystems"]["jenga"] = {
                "enabled": True,
                "gpu_ratio": getattr(self.config, 'jenga_gpu_embedding_ratio', 0.7),
                "allocations": self.stats.get("jenga_allocations", 0),
                "status": "è¿è¡Œä¸­"
            }
        else:
            detailed_stats["subsystems"]["jenga"] = {"enabled": False}
        
        # KVç¼“å­˜ç»Ÿè®¡
        if self.kv_cache_manager:
            kv_stats = {
                "enabled": True,
                "minikv_enabled": getattr(self.config, 'enable_minikv', False),
                "lacache_enabled": getattr(self.config, 'enable_lacache', False),
                "symphony_enabled": getattr(self.config, 'enable_symphony', False),
                "cache_hits": self.stats.get("cache_hits", 0),
                "lacache_hits": self.stats.get("lacache_hits", 0),
                "status": "è¿è¡Œä¸­"
            }
            if getattr(self.config, 'enable_minikv', False):
                kv_stats["minikv_bits"] = getattr(self.config, 'minikv_quantization_bits', 2)
            if getattr(self.config, 'enable_lacache', False):
                kv_stats["lacache_levels"] = getattr(self.config, 'lacache_levels', 3)
            detailed_stats["subsystems"]["kv_cache"] = kv_stats
        else:
            detailed_stats["subsystems"]["kv_cache"] = {"enabled": False}
        
        # HeadInferç»Ÿè®¡
        if self.head_offloader:
            detailed_stats["subsystems"]["headinfer"] = {
                "enabled": True,
                "offload_ratio": getattr(self.config, 'head_offload_ratio', 0.3),
                "memory_saved_mb": self.stats.get("head_offload_saves_mb", 0),
                "status": "è¿è¡Œä¸­"
            }
        else:
            detailed_stats["subsystems"]["headinfer"] = {"enabled": False}
        
        # SYMPHONYç»Ÿè®¡
        if self.symphony_manager:
            detailed_stats["subsystems"]["symphony"] = {
                "enabled": True,
                "window_size": getattr(self.config, 'symphony_window_size', 8),
                "cache_hits": self.stats.get("symphony_cache_hits", 0),
                "status": "è¿è¡Œä¸­"
            }
        else:
            detailed_stats["subsystems"]["symphony"] = {"enabled": False}
        
        return detailed_stats
    
    def log_performance_summary(self):
        """è¾“å‡ºæ€§èƒ½æ‘˜è¦"""
        logger.info("ğŸ“ˆ HMT æ€§èƒ½æ‘˜è¦:")
        logger.info(f"   ğŸ¯ æ€»åˆ†é…æ¬¡æ•°: {self.stats['total_allocations']}")
        logger.info(f"   ğŸ’¾ ç¼“å­˜å‘½ä¸­: {self.stats['cache_hits']}")
        logger.info(f"   ğŸ”„ Offloadæ“ä½œ: {self.stats['offload_operations']}")
        
        if self.vtensor_manager:
            logger.info(f"   ğŸ“¦ vTensoræ“ä½œ: {self.stats['vtensor_operations']}")
        
        if self.jenga_allocator:
            logger.info(f"   ğŸ§© Jengaåˆ†é…: {self.stats['jenga_allocations']}")
        
        if self.head_offloader:
            saved_mb = self.stats.get('head_offload_saves_mb', 0)
            logger.info(f"   ğŸ¯ HeadInferèŠ‚çœ: {saved_mb:.1f}MB")
        
        if hasattr(self, 'symphony_manager') and self.symphony_manager:
            symphony_hits = self.stats.get('symphony_cache_hits', 0)
            logger.info(f"   ğŸµ SYMPHONYç¼“å­˜å‘½ä¸­: {symphony_hits}")
    
    def update_stats(self, operation: str, value: int = 1):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        if operation in self.stats:
            self.stats[operation] += value
            
            # æ¯100æ¬¡æ“ä½œè®°å½•ä¸€æ¬¡æ€§èƒ½æ‘˜è¦
            if self.stats['total_allocations'] % 100 == 0 and self.stats['total_allocations'] > 0:
                self.log_performance_summary()
    
    async def allocate_memory(
        self, 
        size: int, 
        tier: MemoryTier = MemoryTier.GPU_HBM,
        tensor_type: str = "kv_cache"
    ) -> torch.Tensor:
        """
        æ™ºèƒ½å†…å­˜åˆ†é…
        
        Args:
            size: å†…å­˜å¤§å° (bytes)
            tier: ç›®æ ‡å†…å­˜å±‚çº§
            tensor_type: å¼ é‡ç±»å‹ (kv_cache, attention, etc.)
            
        Returns:
            åˆ†é…çš„å¼ é‡
        """
        self.stats["total_allocations"] += 1
        
        try:
            # Step 1: Jengaå¼‚æ„åˆ†é…å†³ç­–
            optimal_tier = await self.jenga_allocator.select_optimal_tier(
                size=size,
                access_pattern=tensor_type,
                current_usage=self._get_memory_usage()
            )
            
            # Step 2: vTensorè™šæ‹Ÿå†…å­˜åˆ†é…
            if optimal_tier == MemoryTier.GPU_HBM:
                tensor = await self.vtensor_manager.allocate_virtual_tensor(
                    size=size,
                    device=self.device
                )
            else:
                # ä½¿ç”¨ä¼ ç»Ÿåˆ†é…ä½œä¸ºfallback
                tensor = self._allocate_fallback(size, optimal_tier)
            
            # Step 3: æ›´æ–°å†…å­˜ä½¿ç”¨ç»Ÿè®¡
            self._update_memory_stats(optimal_tier, size)
            
            logger.debug(f"Allocated {size} bytes to {optimal_tier}")
            return tensor
            
        except Exception as e:
            logger.error(f"Memory allocation failed: {e}")
            raise
    
    async def manage_kv_cache(
        self,
        layer_id: int,
        head_id: int, 
        seq_len: int,
        kv_data: torch.Tensor
    ) -> torch.Tensor:
        """
        æ™ºèƒ½KVç¼“å­˜ç®¡ç†
        
        é›†æˆMiniKVé‡åŒ– + LaCacheé˜¶æ¢¯å½¢ç¼“å­˜ + HeadInfer offloading
        """
        cache_key = f"layer_{layer_id}_head_{head_id}"
        
        try:
            # Step 1: MiniKV 2ä½é‡åŒ–å‹ç¼©
            if self.config.enable_minikv:
                compressed_kv = await self.kv_managers['minikv'].compress(
                    kv_data, layer_id=layer_id, head_id=head_id
                )
            else:
                compressed_kv = kv_data
            
            # Step 2: LaCacheé˜¶æ¢¯å½¢ç¼“å­˜ç­–ç•¥
            if self.config.enable_lacache:
                cached_kv = await self.kv_managers['lacache'].cache_with_ladder(
                    compressed_kv, 
                    seq_len=seq_len,
                    layer_id=layer_id
                )
            else:
                cached_kv = compressed_kv
            
            # Step 3: HeadInferæ™ºèƒ½offloading
            if self.config.enable_head_offload:
                final_kv = await self.head_offloader.smart_offload(
                    cached_kv,
                    head_id=head_id,
                    priority=self._calculate_head_priority(layer_id, head_id)
                )
            else:
                final_kv = cached_kv
            
            # Step 4: SYMPHONYå¤šè½®ä¼˜åŒ–
            if self.config.enable_symphony:
                optimized_kv = await self.symphony_manager.optimize_for_session(
                    final_kv,
                    cache_key=cache_key
                )
            else:
                optimized_kv = final_kv
            
            self.stats["cache_hits"] += 1
            return optimized_kv
            
        except Exception as e:
            self.stats["cache_misses"] += 1
            logger.error(f"KV cache management failed: {e}")
            return kv_data  # è¿”å›åŸå§‹æ•°æ®ä½œä¸ºfallback
    
    def _calculate_head_priority(self, layer_id: int, head_id: int) -> float:
        """è®¡ç®—æ³¨æ„åŠ›å¤´ä¼˜å…ˆçº§ (åŸºäºHeadInferç ”ç©¶)"""
        # åŸºäºå±‚çº§å’Œå¤´éƒ¨é‡è¦æ€§çš„å¯å‘å¼ç®—æ³•
        layer_weight = 1.0 - (layer_id / 32)  # å‡è®¾32å±‚æ¨¡å‹
        head_weight = 1.0  # å¯ä»¥åŸºäºå®é™…ä½¿ç”¨ç»Ÿè®¡è°ƒæ•´
        
        return layer_weight * head_weight
    
    def _get_memory_usage(self) -> Dict[MemoryTier, float]:
        """è·å–å„å±‚å†…å­˜ä½¿ç”¨ç‡"""
        # å®ç°å†…å­˜ä½¿ç”¨ç›‘æ§é€»è¾‘
        usage = {}
        for tier in MemoryTier:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å†…å­˜ç›‘æ§API
            usage[tier] = 0.5  # å ä½ç¬¦
        return usage
    
    def _update_memory_stats(self, tier: MemoryTier, size: int):
        """æ›´æ–°å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        if tier not in self.stats["memory_usage"]:
            self.stats["memory_usage"][tier] = 0
        self.stats["memory_usage"][tier] += size
    
    def _allocate_fallback(self, size: int, tier: MemoryTier) -> torch.Tensor:
        """å¤‡ç”¨å†…å­˜åˆ†é…æ–¹æ¡ˆ"""
        if tier == MemoryTier.GPU_HBM:
            return torch.empty(size // 4, dtype=torch.float32, device=self.device)
        elif tier == MemoryTier.CPU_DRAM:
            return torch.empty(size // 4, dtype=torch.float32, device="cpu")
        else:
            # NVMeç­‰å…¶ä»–å­˜å‚¨çš„å¤„ç†
            return torch.empty(size // 4, dtype=torch.float32, device="cpu")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        cache_hit_rate = (
            self.stats["cache_hits"] / 
            max(1, self.stats["cache_hits"] + self.stats["cache_misses"])
        )
        
        return {
            "cache_hit_rate": cache_hit_rate,
            "total_allocations": self.stats["total_allocations"],
            "offload_operations": self.stats["offload_operations"],
            "memory_usage_by_tier": self.stats["memory_usage"],
            "active_optimizations": {
                "minikv_enabled": self.config.enable_minikv,
                "lacache_enabled": self.config.enable_lacache,
                "head_offload_enabled": self.config.enable_head_offload,
                "symphony_enabled": self.config.enable_symphony,
            }
        }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'thread_pool'):
            self.thread_pool.shutdown(wait=True)
        
        # æ¸…ç†å„ä¸ªå­ç³»ç»Ÿ
        await self.vtensor_manager.cleanup()
        
        logger.info("HMT Manager cleanup completed") 