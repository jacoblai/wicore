# WiCore C++ æ¨ç†å¼•æ“ - ç”Ÿäº§éƒ¨ç½²æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—å°†æŒ‡å¯¼æ‚¨åœ¨ Ubuntu 22.04 + GPU æœåŠ¡å™¨ä¸Šå®Œæ•´éƒ¨ç½² WiCore C++ æ¨ç†å¼•æ“ï¼Œä»æ¨¡å‹è·å–åˆ°æœåŠ¡æµ‹è¯•çš„å…¨æµç¨‹ã€‚

**ç›®æ ‡ç¯å¢ƒ**: Ubuntu 22.04 LTS + NVIDIA GPU  
**æµ‹è¯•æ¨¡å‹**: Google Gemma-3-27B-IT  
**é¢„æœŸæ€§èƒ½**: 150+ tokens/s @ RTX 4090, 128Kä¸Šä¸‹æ–‡æ”¯æŒ

---

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA RTX 3090/4090 æˆ–æ›´é«˜ (â‰¥24GB VRAM)
- **CPU**: Intel/AMD 8æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**: 64GB RAM æ¨è (32GB æœ€ä½)
- **å­˜å‚¨**: 500GB å¯ç”¨ç©ºé—´ (NVMe SSD æ¨è)

### è½¯ä»¶ç¯å¢ƒ
- Ubuntu 22.04 LTS
- NVIDIA Driver â‰¥ 525.x
- CUDA 12.0+
- TensorRT 8.6+
- Docker (å¯é€‰)

---

## ğŸ”§ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

### 1.1 æ›´æ–°ç³»ç»Ÿ
```bash
sudo apt update && sudo apt upgrade -y
sudo apt install -y wget curl git build-essential cmake ninja-build
```

### 1.2 å®‰è£… NVIDIA é©±åŠ¨
```bash
# æ£€æŸ¥æ˜¾å¡
lspci | grep -i nvidia

# å®‰è£…é©±åŠ¨ (å»ºè®®ä½¿ç”¨å®˜æ–¹é©±åŠ¨)
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
sudo apt update
sudo apt install -y nvidia-driver-535 nvidia-dkms-535

# é‡å¯ç³»ç»Ÿ
sudo reboot
```

### 1.3 éªŒè¯é©±åŠ¨å®‰è£…
```bash
nvidia-smi
# åº”è¯¥çœ‹åˆ° GPU ä¿¡æ¯å’Œé©±åŠ¨ç‰ˆæœ¬
```

### 1.4 å®‰è£… CUDA 12.2
```bash
# ä¸‹è½½ CUDA Toolkit
wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/cuda_12.2.0_535.54.03_linux.run
sudo sh cuda_12.2.0_535.54.03_linux.run

# æ·»åŠ ç¯å¢ƒå˜é‡
echo 'export PATH=/usr/local/cuda-12.2/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# éªŒè¯å®‰è£…
nvcc --version
```

### 1.5 å®‰è£… TensorRT 8.6
```bash
# ä¸‹è½½ TensorRT (éœ€è¦NVIDIAå¼€å‘è€…è´¦å·)
# æ–¹æ³•1: é€šè¿‡ APT (æ¨è)
sudo apt install -y tensorrt

# æ–¹æ³•2: æ‰‹åŠ¨ä¸‹è½½å®‰è£…
# ä» https://developer.nvidia.com/tensorrt ä¸‹è½½ TensorRT-8.6.x.x.Ubuntu-22.04.x86_64-gnu.cuda-12.0.tar.gz
# tar -xzf TensorRT-8.6.x.x.Ubuntu-22.04.x86_64-gnu.cuda-12.0.tar.gz
# sudo cp -r TensorRT-8.6.x.x/* /usr/local/
# echo 'export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH' >> ~/.bashrc

# éªŒè¯ TensorRT
python3 -c "import tensorrt as trt; print(f'TensorRT version: {trt.__version__}')"
```

---

## ğŸ“¦ ç¬¬äºŒæ­¥ï¼šä¾èµ–åº“å®‰è£…

### 2.1 å®‰è£…åŸºç¡€ä¾èµ–
```bash
sudo apt install -y \
    libopencv-dev \
    libjsoncpp-dev \
    libevhtp-dev \
    libevent-dev \
    pkg-config \
    libsentencepiece-dev \
    protobuf-compiler \
    libprotobuf-dev
```

### 2.2 æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬
```bash
pkg-config --modversion opencv4  # >= 4.5
pkg-config --modversion jsoncpp  # >= 1.9
pkg-config --modversion sentencepiece  # >= 0.1.96
```

### 2.3 å®‰è£… Python å·¥å…· (å¯é€‰)
```bash
sudo apt install -y python3-pip
pip3 install torch torchvision transformers accelerate
```

---

## ğŸ¤– ç¬¬ä¸‰æ­¥ï¼šæ¨¡å‹è·å–å’Œå‡†å¤‡

### 3.1 åˆ›å»ºå·¥ä½œç›®å½•
```bash
mkdir -p ~/wicore_deployment/models
cd ~/wicore_deployment
```

### 3.2 æ–¹æ³•ä¸€ï¼šä½¿ç”¨ Hugging Face Hub (æ¨è)
```bash
# å®‰è£… huggingface-hub
pip3 install huggingface-hub

# ä¸‹è½½ Gemma-3-27B-IT æ¨¡å‹
huggingface-cli download google/gemma-3-27b-it \
    --local-dir ./models/gemma-3-27b-it \
    --local-dir-use-symlinks False

# æ¨¡å‹æ–‡ä»¶ç»“æ„åº”è¯¥æ˜¯:
# models/gemma-3-27b-it/
# â”œâ”€â”€ config.json
# â”œâ”€â”€ model.safetensors.index.json
# â”œâ”€â”€ model-00001-of-00055.safetensors
# â”œâ”€â”€ ...
# â”œâ”€â”€ tokenizer.model
# â””â”€â”€ tokenizer_config.json
```

### 3.3 æ–¹æ³•äºŒï¼šæ‰‹åŠ¨ä¸‹è½½ (å¦‚æœHubè®¿é—®å›°éš¾)
```bash
# ä½¿ç”¨ git-lfs å…‹éš†æ¨¡å‹
git lfs install
git clone https://huggingface.co/google/gemma-3-27b-it ./models/gemma-3-27b-it

# æˆ–è€…ä½¿ç”¨é•œåƒç«™ç‚¹
git clone https://hf-mirror.com/google/gemma-3-27b-it ./models/gemma-3-27b-it
```

### 3.4 éªŒè¯æ¨¡å‹æ–‡ä»¶
```bash
ls -la ./models/gemma-3-27b-it/
du -sh ./models/gemma-3-27b-it/  # åº”è¯¥çº¦ 50-60GB

# æ£€æŸ¥ tokenizer
python3 -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('./models/gemma-3-27b-it')
print(f'Tokenizer vocab size: {tokenizer.vocab_size}')
print('Tokenizer loaded successfully!')
"
```

### 3.5 å‡†å¤‡ TensorRT å¼•æ“ç¼“å­˜ç›®å½•
```bash
mkdir -p ./models/gemma-3-27b-it/engine_cache
chmod 755 ./models/gemma-3-27b-it/engine_cache
```

---

## ğŸ› ï¸ ç¬¬å››æ­¥ï¼šä»£ç ç¼–è¯‘

### 4.1 å…‹éš†é¡¹ç›®ä»£ç 
```bash
cd ~/wicore_deployment
git clone <YOUR_REPO_URL> wicore_src
# æˆ–è€…ä¸Šä¼ ä½ çš„ä»£ç åŒ…
# scp -r /path/to/wicore user@server:~/wicore_deployment/wicore_src
```

### 4.2 ç¼–è¯‘é¡¹ç›®
```bash
cd wicore_src

# åˆ›å»ºæ„å»ºç›®å½•
mkdir -p build && cd build

# é…ç½® CMake
cmake .. \
    -DCMAKE_BUILD_TYPE=Release \
    -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-12.2 \
    -DTensorRT_ROOT=/usr/local/tensorrt \
    -DCMAKE_CUDA_ARCHITECTURES="75;80;86;89"

# ç¼–è¯‘ (ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ)
make -j$(nproc)

# éªŒè¯ç¼–è¯‘ç»“æœ
ls -la wicore_server
ldd wicore_server  # æ£€æŸ¥åŠ¨æ€åº“ä¾èµ–
```

### 4.3 è§£å†³å¸¸è§ç¼–è¯‘é—®é¢˜
```bash
# å¦‚æœ TensorRT è·¯å¾„ä¸å¯¹
export TensorRT_ROOT=/usr/lib/x86_64-linux-gnu  # APT å®‰è£…è·¯å¾„

# å¦‚æœ CUDA æ¶æ„ä¸åŒ¹é…
# RTX 3090: 86, RTX 4090: 89, A100: 80
cmake .. -DCMAKE_CUDA_ARCHITECTURES="89"

# å¦‚æœé“¾æ¥é”™è¯¯
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib:$LD_LIBRARY_PATH
```

---

## âš™ï¸ ç¬¬äº”æ­¥ï¼šé…ç½®æ–‡ä»¶å‡†å¤‡

### 5.1 åˆ›å»ºç”Ÿäº§é…ç½®
```bash
cd ~/wicore_deployment
cp wicore_src/config_template.json production_config.json
```

### 5.2 ç¼–è¾‘é…ç½®æ–‡ä»¶
```bash
nano production_config.json
```

**å…³é”®é…ç½®é¡¹**:
```json
{
  "model_path": "/home/username/wicore_deployment/models/gemma-3-27b-it",
  "tokenizer_path": "/home/username/wicore_deployment/models/gemma-3-27b-it/tokenizer.model",
  "server_port": 8080,
  "max_batch_size": 16,
  "max_context_length": 131072,
  "max_concurrent_requests": 32,
  
  "gpu_memory_gb": 20,
  "cpu_memory_gb": 32,
  "nvme_cache_path": "/tmp/wicore_cache",
  
  "trt_precision": "fp16",
  "trt_max_workspace_gb": 8,
  "trt_enable_sparse": true,
  
  "image_resolution": 896,
  "image_preprocessing_threads": 4,
  
  "enable_performance_logging": true,
  "log_level": "info"
}
```

### 5.3 åˆ›å»ºç¼“å­˜ç›®å½•
```bash
sudo mkdir -p /tmp/wicore_cache
sudo chmod 777 /tmp/wicore_cache
```

### 5.4 åˆ›å»ºé™æ€æ–‡ä»¶ç›®å½• (å¯é€‰)
```bash
mkdir -p static
echo '<h1>WiCore Inference Engine</h1><p>Server is running!</p>' > static/index.html
```

---

## ğŸš€ ç¬¬å…­æ­¥ï¼šæœåŠ¡å¯åŠ¨

### 6.1 é¦–æ¬¡å¯åŠ¨æµ‹è¯•
```bash
cd ~/wicore_deployment

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
export OMP_NUM_THREADS=8

# å¯åŠ¨æœåŠ¡ (å‰å°æ¨¡å¼ï¼Œæ–¹ä¾¿è°ƒè¯•)
./wicore_src/build/wicore_server production_config.json
```

**é¢„æœŸè¾“å‡º**:
```
WiCore Engine starting...
Loading configuration from production_config.json
HMT Memory Manager initialized
MultiModal Processor initialized  
TensorRT Inference Engine initialized
Batch Scheduler initialized
Web Server initialized
Server started on 0.0.0.0:8080
Ready to accept requests!
```

### 6.2 åå°æœåŠ¡å¯åŠ¨
```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# åå°å¯åŠ¨
nohup ./wicore_src/build/wicore_server production_config.json > logs/wicore.log 2>&1 &

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep wicore_server

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/wicore.log
```

### 6.3 åˆ›å»ºç³»ç»ŸæœåŠ¡ (å¯é€‰)
```bash
sudo tee /etc/systemd/system/wicore.service > /dev/null <<EOF
[Unit]
Description=WiCore Inference Engine
After=network.target

[Service]
Type=simple
User=wicore
WorkingDirectory=/home/wicore/wicore_deployment
ExecStart=/home/wicore/wicore_deployment/wicore_src/build/wicore_server production_config.json
Restart=always
RestartSec=10
Environment=CUDA_VISIBLE_DEVICES=0
Environment=OMP_NUM_THREADS=8

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable wicore
sudo systemctl start wicore
sudo systemctl status wicore
```

---

## ğŸ§ª ç¬¬ä¸ƒæ­¥ï¼šåŠŸèƒ½æµ‹è¯•

### 7.1 åŸºç¡€è¿é€šæ€§æµ‹è¯•
```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8080/health

# æ¨¡å‹åˆ—è¡¨
curl http://localhost:8080/v1/models

# ç³»ç»ŸçŠ¶æ€
curl http://localhost:8080/v1/status
```

### 7.2 èŠå¤©å®Œæˆæµ‹è¯•
```bash
# ç®€å•æ–‡æœ¬å¯¹è¯
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma-3-27b-it",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ],
    "max_tokens": 150,
    "temperature": 0.7
  }'
```

### 7.3 æµå¼è¾“å‡ºæµ‹è¯•
```bash
# æµå¼å¯¹è¯
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma-3-27b-it",
    "messages": [
      {"role": "user", "content": "Write a short story about AI"}
    ],
    "max_tokens": 300,
    "stream": true
  }'
```

### 7.4 å¤šæ¨¡æ€æµ‹è¯• (å›¾åƒ+æ–‡æœ¬)
```bash
# å‡†å¤‡æµ‹è¯•å›¾åƒ
wget https://example.com/test_image.jpg -O test_image.jpg

# å¤šæ¨¡æ€å¯¹è¯ (Base64ç¼–ç )
base64_image=$(base64 -w 0 test_image.jpg)

curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma-3-27b-it",
    "messages": [
      {
        "role": "user", 
        "content": [
          {"type": "text", "text": "What is in this image?"},
          {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,'$base64_image'"}}
        ]
      }
    ],
    "max_tokens": 200
  }'
```

### 7.5 å‹åŠ›æµ‹è¯•
```bash
# å®‰è£…æµ‹è¯•å·¥å…·
sudo apt install -y apache2-utils

# å¹¶å‘æµ‹è¯•
ab -n 100 -c 10 -H "Content-Type: application/json" \
   -p test_payload.json \
   http://localhost:8080/v1/chat/completions

# åˆ›å»ºæµ‹è¯•è½½è·
echo '{
  "model": "gemma-3-27b-it",
  "messages": [{"role": "user", "content": "Test message"}],
  "max_tokens": 50
}' > test_payload.json
```

---

## ğŸ“Š ç¬¬å…«æ­¥ï¼šæ€§èƒ½ç›‘æ§

### 8.1 ç³»ç»Ÿç›‘æ§
```bash
# GPU ä½¿ç”¨ç‡
watch -n 1 nvidia-smi

# å†…å­˜ä½¿ç”¨
watch -n 1 'free -h && df -h'

# ç½‘ç»œè¿æ¥
ss -tulpn | grep :8080
```

### 8.2 åº”ç”¨ç›‘æ§
```bash
# æ€§èƒ½æŒ‡æ ‡
curl http://localhost:8080/metrics

# å®æ—¶æ—¥å¿—
tail -f logs/wicore.log

# è¿›ç¨‹èµ„æºä½¿ç”¨
top -p $(pgrep wicore_server)
```

### 8.3 åŸºå‡†æµ‹è¯•
```bash
# åˆ›å»ºåŸºå‡†æµ‹è¯•è„šæœ¬
cat > benchmark.py << 'EOF'
#!/usr/bin/env python3
import time
import requests
import threading
import statistics

def benchmark_request():
    start_time = time.time()
    response = requests.post('http://localhost:8080/v1/chat/completions', 
        json={
            "model": "gemma-3-27b-it",
            "messages": [{"role": "user", "content": "Count from 1 to 10"}],
            "max_tokens": 100
        })
    latency = time.time() - start_time
    
    if response.status_code == 200:
        data = response.json()
        tokens = data.get('usage', {}).get('completion_tokens', 0)
        return latency, tokens
    return latency, 0

# è¿è¡ŒåŸºå‡†æµ‹è¯•
latencies = []
token_counts = []

print("Running benchmark (20 requests)...")
for i in range(20):
    latency, tokens = benchmark_request()
    latencies.append(latency)
    token_counts.append(tokens)
    print(f"Request {i+1}: {latency:.2f}s, {tokens} tokens")

# ç»Ÿè®¡ç»“æœ
avg_latency = statistics.mean(latencies)
avg_tokens = statistics.mean(token_counts)
tokens_per_second = avg_tokens / avg_latency if avg_latency > 0 else 0

print(f"\nBenchmark Results:")
print(f"Average Latency: {avg_latency:.2f}s")
print(f"Average Tokens: {avg_tokens:.1f}")
print(f"Tokens/Second: {tokens_per_second:.1f}")
EOF

python3 benchmark.py
```

---

## ğŸ”§ ç¬¬ä¹æ­¥ï¼šæ•…éšœæ’é™¤

### 9.1 å¸¸è§é—®é¢˜

**é—®é¢˜**: CUDA out of memory
```bash
# è§£å†³æ–¹æ¡ˆ
# 1. é™ä½ max_batch_size
# 2. é™ä½ gpu_memory_gb
# 3. ä½¿ç”¨ TensorRT é‡åŒ–
```

**é—®é¢˜**: TensorRT å¼•æ“æ„å»ºå¤±è´¥
```bash
# æ£€æŸ¥ CUDA æ¶æ„åŒ¹é…
nvidia-smi --query-gpu=name,compute_cap --format=csv

# é‡æ–°æ„å»ºå¼•æ“
rm -rf models/gemma-3-27b-it/engine_cache/*
```

**é—®é¢˜**: æ¨¡å‹åŠ è½½æ…¢
```bash
# ä½¿ç”¨ NVMe å­˜å‚¨
# é¢„å…ˆæ„å»º TensorRT å¼•æ“
# å¯ç”¨æ¨¡å‹å¹¶è¡ŒåŠ è½½
```

### 9.2 æ—¥å¿—åˆ†æ
```bash
# é”™è¯¯æ—¥å¿—
grep -i error logs/wicore.log

# æ€§èƒ½æ—¥å¿—
grep -i "tokens/s\|latency\|throughput" logs/wicore.log

# å†…å­˜æ—¥å¿—
grep -i "memory\|oom\|allocation" logs/wicore.log
```

### 9.3 è°ƒè¯•æ¨¡å¼
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
./wicore_src/build/wicore_server production_config.json --log-level debug

# ä½¿ç”¨ gdb è°ƒè¯•
gdb ./wicore_src/build/wicore_server
(gdb) run production_config.json
```

---

## ğŸ“ˆ ç¬¬åæ­¥ï¼šæ€§èƒ½ä¼˜åŒ–

### 10.1 TensorRT ä¼˜åŒ–
```bash
# ä¿®æ”¹é…ç½®å¯ç”¨æ›´å¤šä¼˜åŒ–
"trt_enable_fp16": true,
"trt_enable_int8": false,  # éœ€è¦æ ¡å‡†æ•°æ®é›†
"trt_enable_sparse": true,
"trt_enable_refit": true,
"trt_optimization_level": 5
```

### 10.2 ç³»ç»Ÿè°ƒä¼˜
```bash
# CPU è°ƒä¼˜
echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# å†…å­˜è°ƒä¼˜
echo 1 | sudo tee /proc/sys/vm/swappiness
echo 3 | sudo tee /proc/sys/vm/drop_caches

# ç½‘ç»œè°ƒä¼˜
echo 'net.core.rmem_max = 16777216' | sudo tee -a /etc/sysctl.conf
echo 'net.core.wmem_max = 16777216' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

### 10.3 GPU è°ƒä¼˜
```bash
# è®¾ç½® GPU æ€§èƒ½æ¨¡å¼
sudo nvidia-smi -pm 1
sudo nvidia-smi -ac 1215,210  # æ ¹æ®GPUå‹å·è°ƒæ•´
```

---

## ğŸ¯ é¢„æœŸæ€§èƒ½æŒ‡æ ‡

### RTX 4090 + Gemma-3-27B-IT
- **ååé‡**: 150-200 tokens/s
- **å»¶è¿Ÿ**: 50-100ms (é¦–token)
- **å¹¶å‘**: 32 å¹¶å‘è¯·æ±‚
- **å†…å­˜ä½¿ç”¨**: GPU 20GB, CPU 16GB
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 128K tokens

### RTX 3090 + Gemma-3-27B-IT  
- **ååé‡**: 100-150 tokens/s
- **å»¶è¿Ÿ**: 80-150ms (é¦–token)
- **å¹¶å‘**: 16 å¹¶å‘è¯·æ±‚
- **å†…å­˜ä½¿ç”¨**: GPU 22GB, CPU 16GB
- **ä¸Šä¸‹æ–‡é•¿åº¦**: 64K tokens

---

## âœ… å®Œæˆæ£€æŸ¥æ¸…å•

- [ ] Ubuntu 22.04 ç¯å¢ƒå‡†å¤‡
- [ ] NVIDIA é©±åŠ¨å®‰è£… (â‰¥525.x)
- [ ] CUDA 12.2 å®‰è£…å’Œé…ç½®
- [ ] TensorRT 8.6 å®‰è£…å’ŒéªŒè¯
- [ ] ä¾èµ–åº“å®‰è£…å®Œæˆ
- [ ] Gemma-3-27B-IT æ¨¡å‹ä¸‹è½½
- [ ] WiCore æºç ç¼–è¯‘æˆåŠŸ
- [ ] ç”Ÿäº§é…ç½®æ–‡ä»¶å‡†å¤‡
- [ ] æœåŠ¡å¯åŠ¨å’Œè¿è¡Œ
- [ ] API åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ
- [ ] ç›‘æ§ç³»ç»Ÿé…ç½®
- [ ] æ•…éšœæ’é™¤æ–‡æ¡£ç†Ÿæ‚‰

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨éƒ¨ç½²è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **ç³»ç»Ÿä¿¡æ¯**: `uname -a`, `nvidia-smi`, `nvcc --version`
2. **é”™è¯¯æ—¥å¿—**: å…·ä½“çš„é”™è¯¯ä¿¡æ¯å’Œæ—¥å¿—
3. **é…ç½®æ–‡ä»¶**: ä½¿ç”¨çš„é…ç½®å‚æ•°
4. **ç¡¬ä»¶è§„æ ¼**: GPUå‹å·ã€å†…å­˜å¤§å°ç­‰

**éƒ¨ç½²å®Œæˆåï¼Œæ‚¨çš„ WiCore æ¨ç†å¼•æ“å³å¯ä¸ºç”Ÿäº§ç¯å¢ƒæä¾›é«˜æ€§èƒ½çš„å¤šæ¨¡æ€AIæœåŠ¡ï¼** ğŸš€ 