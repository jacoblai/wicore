#!/bin/bash

# WiCore Mojo æ¨ç†å¼•æ“ç¯å¢ƒæ­å»ºè„šæœ¬
# ä½¿ç”¨ Pixi ç®¡ç† Modular å¹³å°ï¼ˆå®˜æ–¹æ¨èæ–¹å¼ï¼‰

set -e

echo "ğŸš€ å¼€å§‹æ­å»º WiCore Mojo æ¨ç†å¼•æ“ç¯å¢ƒ..."

# æ£€æµ‹æ“ä½œç³»ç»Ÿ
OS_TYPE=$(uname -s)
echo "ğŸ–¥ï¸  æ£€æµ‹åˆ°æ“ä½œç³»ç»Ÿ: $OS_TYPE"

# åˆå§‹åŒ–é…ç½®å˜é‡
ENABLE_MULTI_GPU=false
TARGET_DEVICES='"cpu:0"'

# æ£€æŸ¥ NVIDIA GPU ç¯å¢ƒ
echo "ğŸ” æ£€æŸ¥ GPU ç¯å¢ƒ..."
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… NVIDIA GPU é©±åŠ¨å·²å®‰è£…"
    nvidia-smi --query-gpu=name,memory.total --format=csv,noheader
    
    # æ£€æŸ¥ GPU æ•°é‡
    GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
    echo "ğŸ“Š æ£€æµ‹åˆ° $GPU_COUNT ä¸ª GPU"
    
    if [ "$GPU_COUNT" -gt 1 ]; then
        ENABLE_MULTI_GPU=true
        TARGET_DEVICES='"gpu:0", "gpu:1"'
        echo "ğŸ¯ å¤šGPUé…ç½®: $GPU_COUNT å¡"
    else
        ENABLE_MULTI_GPU=false
        TARGET_DEVICES='"gpu:0"'
        echo "ğŸ¯ å•GPUé…ç½®"
    fi
elif [ "$OS_TYPE" = "Darwin" ]; then
    echo "ğŸ macOS ç¯å¢ƒ - å°†ä½¿ç”¨ CPU æ¨¡æ‹Ÿæ¨¡å¼"
    TARGET_DEVICES='"cpu:0"'
else
    echo "âš ï¸  æœªæ£€æµ‹åˆ° NVIDIA GPUï¼Œå°†ä½¿ç”¨ CPU æ¨¡æ‹Ÿæ¨¡å¼"
    TARGET_DEVICES='"cpu:0"'
fi

# å®‰è£… Pixiï¼ˆModular å®˜æ–¹æ¨èçš„åŒ…ç®¡ç†å™¨ï¼‰
echo "ğŸ“¦ å®‰è£… Pixi åŒ…ç®¡ç†å™¨..."
if ! command -v pixi &> /dev/null; then
    echo "æ­£åœ¨ä¸‹è½½ Pixi..."
    curl -fsSL https://pixi.sh/install.sh | bash
    
    # æ·»åŠ åˆ°å½“å‰ä¼šè¯çš„ PATH
    export PATH="$HOME/.pixi/bin:$PATH"
    
    echo "âœ… Pixi å®‰è£…å®Œæˆ"
else
    echo "âœ… Pixi å·²å®‰è£…"
fi

# é…ç½® Pixi é»˜è®¤æ¸ é“ï¼ˆåŒ…å« Modular æ¸ é“ï¼‰
echo "âš™ï¸  é…ç½® Pixi é»˜è®¤æ¸ é“..."
mkdir -p "$HOME/.pixi"
echo 'default-channels = ["https://conda.modular.com/max-nightly", "conda-forge"]' > "$HOME/.pixi/config.toml"
echo "âœ… Pixi æ¸ é“é…ç½®å®Œæˆ"

# åˆå§‹åŒ– WiCore é¡¹ç›®
echo "ğŸ—ï¸  åˆå§‹åŒ– WiCore é¡¹ç›®..."
if [ ! -f "pixi.toml" ]; then
    pixi init . --name wicore-mojo
    echo "âœ… é¡¹ç›®åˆå§‹åŒ–å®Œæˆ"
else
    echo "âœ… é¡¹ç›®å·²å­˜åœ¨"
fi

# å®‰è£… Modular å¹³å°
echo "ğŸ”§ å®‰è£… Modular å¹³å°..."
pixi add "modular=*" "python==3.11" || {
    echo "âš ï¸  Modular å®‰è£…å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç¨³å®šç‰ˆæœ¬..."
    pixi add "modular~=25.4" "python==3.11"
}

# å®‰è£… Python ä¾èµ–
echo "ğŸ“¦ å®‰è£… Python ä¾èµ–..."
pixi add torch torchvision torchaudio transformers accelerate
pixi add fastapi uvicorn pydantic
pixi add numpy pandas psutil requests
pixi add matplotlib seaborn

echo "âœ… ä¾èµ–å®‰è£…å®Œæˆ"

# éªŒè¯ Modular å®‰è£…
echo "ğŸ§ª éªŒè¯ Modular å®‰è£…..."
pixi run python -c "
import sys
print(f'Python ç‰ˆæœ¬: {sys.version}')

try:
    # æ£€æŸ¥ Modular æ˜¯å¦å¯ç”¨
    import subprocess
    result = subprocess.run(['modular', '--version'], capture_output=True, text=True)
    if result.returncode == 0:
        print(f'âœ… Modular ç‰ˆæœ¬: {result.stdout.strip()}')
    else:
        print('âš ï¸  Modular å‘½ä»¤è¡Œå·¥å…·æœªæ­£ç¡®å®‰è£…')
except Exception as e:
    print(f'âš ï¸  Modular éªŒè¯å¼‚å¸¸: {e}')

# æ£€æŸ¥åŸºç¡€ Python åŒ…
try:
    import torch
    print(f'âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}')
    print(f'âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}')
    if torch.cuda.is_available():
        print(f'âœ… GPU æ•°é‡: {torch.cuda.device_count()}')
except ImportError:
    print('âŒ PyTorch æœªæ­£ç¡®å®‰è£…')

try:
    import transformers
    print(f'âœ… Transformers ç‰ˆæœ¬: {transformers.__version__}')
except ImportError:
    print('âŒ Transformers æœªæ­£ç¡®å®‰è£…')
"

# åˆ›å»ºç¯å¢ƒé…ç½®
echo "âš™ï¸  åˆ›å»ºç¯å¢ƒé…ç½®..."
mkdir -p configs

cat > configs/production.json << EOF
{
    "model_path": "models/gemma-3-27b-it",
    "server_port": 8000,
    "max_batch_size": 16,
    "max_context_length": 131072,
    "gpu_memory_limit_gb": 15.0,
    "enable_multi_gpu": $ENABLE_MULTI_GPU,
    "target_devices": [$TARGET_DEVICES],
    "hmt_config": {
        "enable_a2cr": true,
        "nvme_cache_path": "/tmp/wicore_cache",
        "time_decay_factor": 0.05,
        "attention_weight": 0.4,
        "frequency_weight": 0.3,
        "recency_weight": 0.3,
        "max_cache_size_gb": 100
    },
    "performance_config": {
        "enable_batching": true,
        "batch_timeout_ms": 50,
        "max_concurrent_requests": 32,
        "request_timeout_seconds": 30,
        "enable_streaming": true
    },
    "logging": {
        "level": "INFO",
        "enable_request_logging": true,
        "enable_performance_logging": true,
        "log_file": "logs/wicore.log"
    }
}
EOF

# åˆ›å»ºå¼€å‘é…ç½®
cat > configs/development.json << EOF
{
    "model_path": "models/gemma-3-27b-it",
    "server_port": 8000,
    "max_batch_size": 8,
    "max_context_length": 32768,
    "gpu_memory_limit_gb": 8.0,
    "enable_multi_gpu": false,
    "target_devices": ["cpu:0"],
    "simulation_mode": true,
    "hmt_config": {
        "enable_a2cr": true,
        "nvme_cache_path": "./cache",
        "time_decay_factor": 0.05,
        "attention_weight": 0.4,
        "frequency_weight": 0.3,
        "recency_weight": 0.3
    },
    "logging": {
        "level": "DEBUG",
        "enable_request_logging": true,
        "enable_performance_logging": true
    }
}
EOF

echo "âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º"

# åˆ›å»ºå¿…è¦ç›®å½•
echo "ğŸ“ åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„..."
mkdir -p {models,cache,logs,build}

# åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰
echo "ğŸ­ åˆ›å»º MAX Engine æ¨¡æ‹Ÿç¯å¢ƒ..."
mkdir -p simulation

cat > simulation/max_simulation.py << 'EOF'
"""
MAX Engine æ¨¡æ‹Ÿç¯å¢ƒ
ç”¨äºåœ¨æ²¡æœ‰çœŸå® MAX Engine çš„æƒ…å†µä¸‹è¿›è¡Œå¼€å‘æµ‹è¯•
"""

import numpy as np
import time
import threading
from typing import List, Dict, Optional, Any
import psutil


class Device:
    """æ¨¡æ‹Ÿè®¡ç®—è®¾å¤‡"""
    def __init__(self, device_type: str, device_id: str, memory_total: int = 16):
        self.type = device_type
        self.id = device_id
        self.memory_total = memory_total * 1024 * 1024 * 1024  # GB -> Bytes
        self.memory_allocated = 0
        self.memory_available = self.memory_total
        self.compute_capability = 7.5 if device_type == "gpu" else 1.0
        self.is_available = True
        
    def allocate_memory(self, size: int) -> bool:
        """åˆ†é…å†…å­˜"""
        if self.memory_available >= size:
            self.memory_allocated += size
            self.memory_available -= size
            return True
        return False
    
    def free_memory(self, size: int):
        """é‡Šæ”¾å†…å­˜"""
        self.memory_allocated = max(0, self.memory_allocated - size)
        self.memory_available = min(self.memory_total, self.memory_available + size)
    
    def get_memory_info(self) -> Dict[str, int]:
        """è·å–å†…å­˜ä¿¡æ¯"""
        return {
            "total": self.memory_total,
            "allocated": self.memory_allocated,
            "available": self.memory_available
        }


class Tensor:
    """æ¨¡æ‹Ÿå¼ é‡"""
    def __init__(self, shape: tuple, dtype: str = "float16", device: str = "cpu:0"):
        self.shape = shape
        self.dtype = dtype
        self.device = device
        self.data = np.random.randn(*shape).astype(self._numpy_dtype())
        self.size = self.data.nbytes
        
    def _numpy_dtype(self):
        """è½¬æ¢æ•°æ®ç±»å‹"""
        dtype_map = {
            "float16": np.float16,
            "float32": np.float32,
            "int32": np.int32,
            "int8": np.int8
        }
        return dtype_map.get(self.dtype, np.float16)
    
    def to(self, device: str):
        """ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        self.device = device
        return self
    
    def __str__(self):
        return f"Tensor(shape={self.shape}, dtype={self.dtype}, device={self.device})"


class ModelGraph:
    """æ¨¡æ‹Ÿè®¡ç®—å›¾"""
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.layers = []
        self.parameters = {}
        self.memory_requirement = 0
        
    def add_layer(self, layer_name: str, params: Dict):
        """æ·»åŠ å±‚"""
        self.layers.append({"name": layer_name, "params": params})
        
    def execute(self, inputs: List[Tensor], device_ids: List[str]) -> List[Tensor]:
        """æ‰§è¡Œè®¡ç®—å›¾"""
        # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
        batch_size = inputs[0].shape[0] if inputs else 1
        base_latency = 0.01  # 10msåŸºç¡€å»¶è¿Ÿ
        compute_latency = batch_size * 0.005  # æ¯ä¸ªsample 5ms
        
        time.sleep(base_latency + compute_latency)
        
        # æ¨¡æ‹Ÿè¾“å‡º
        if inputs:
            input_tensor = inputs[0]
            # æ¨¡æ‹Ÿè¯­è¨€æ¨¡å‹è¾“å‡ºï¼ˆvocab_size=32000ï¼‰
            output_shape = (input_tensor.shape[0], input_tensor.shape[1], 32000)
            output = Tensor(output_shape, "float16", input_tensor.device)
            return [output]
        else:
            return [Tensor((1, 1, 32000), "float16", "cpu:0")]


class Engine:
    """æ¨¡æ‹Ÿ MAX Engine"""
    
    def __init__(self):
        self.devices = []
        self.loaded_models = {}
        
    def discover_devices(self) -> List[Device]:
        """å‘ç°è®¾å¤‡"""
        if not self.devices:
            # æ¨¡æ‹Ÿ CPU è®¾å¤‡
            cpu_count = psutil.cpu_count(logical=False)
            for i in range(min(cpu_count, 2)):  # æœ€å¤šæ¨¡æ‹Ÿ2ä¸ªCPUæ ¸å¿ƒ
                device = Device("cpu", f"cpu:{i}", memory_total=8)
                self.devices.append(device)
            
            # åœ¨ç”Ÿäº§ç¯å¢ƒä¸‹ä¼šå‘ç° GPU
            # è¿™é‡Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶æ˜¯å¦æ¨¡æ‹Ÿ GPU
            import os
            if os.getenv("WICORE_SIMULATE_GPU", "false").lower() == "true":
                for i in range(2):
                    device = Device("gpu", f"gpu:{i}", memory_total=16)
                    self.devices.append(device)
        
        return self.devices.copy()
    
    def allocate_device_memory(self, device_id: str, size: int) -> Optional[int]:
        """åœ¨æŒ‡å®šè®¾å¤‡ä¸Šåˆ†é…å†…å­˜"""
        device = self._get_device(device_id)
        if device and device.allocate_memory(size):
            # è¿”å›æ¨¡æ‹Ÿçš„å†…å­˜åœ°å€
            return id(device) + size
        return None
    
    def free_device_memory(self, device_id: str, ptr: int, size: int):
        """é‡Šæ”¾è®¾å¤‡å†…å­˜"""
        device = self._get_device(device_id)
        if device:
            device.free_memory(size)
    
    def load_model(self, model_path: str, device_ids: List[str]) -> ModelGraph:
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ¤– [æ¨¡æ‹Ÿ] åŠ è½½æ¨¡å‹: {model_path}")
        
        # æ¨¡æ‹Ÿæ¨¡å‹åŠ è½½æ—¶é—´
        time.sleep(2.0)  # 2ç§’åŠ è½½æ—¶é—´
        
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ¨¡å‹å›¾
        graph = ModelGraph(model_path)
        
        # æ¨¡æ‹Ÿ Gemma-3-27B çš„ç»“æ„
        for i in range(32):  # 32å±‚ Transformer
            graph.add_layer(f"transformer_layer_{i}", {
                "hidden_size": 4096,
                "num_heads": 32,
                "intermediate_size": 11008
            })
        
        # ä¼°ç®—å†…å­˜éœ€æ±‚ï¼ˆç®€åŒ–ï¼‰
        graph.memory_requirement = 27 * 1024 * 1024 * 1024  # 27GB
        
        # æ£€æŸ¥è®¾å¤‡å†…å­˜
        total_available = sum(
            self._get_device(device_id).memory_available 
            for device_id in device_ids 
            if self._get_device(device_id)
        )
        
        if total_available < graph.memory_requirement:
            print(f"âš ï¸  [æ¨¡æ‹Ÿ] å†…å­˜ä¸è¶³ï¼šéœ€è¦ {graph.memory_requirement/1e9:.1f}GBï¼Œå¯ç”¨ {total_available/1e9:.1f}GB")
        
        self.loaded_models[model_path] = graph
        print(f"âœ… [æ¨¡æ‹Ÿ] æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
        return graph
    
    def execute_graph(self, graph: ModelGraph, inputs: List[Tensor], device_ids: List[str]) -> List[Tensor]:
        """æ‰§è¡Œè®¡ç®—å›¾"""
        return graph.execute(inputs, device_ids)
    
    def _get_device(self, device_id: str) -> Optional[Device]:
        """è·å–è®¾å¤‡"""
        for device in self.devices:
            if device.id == device_id:
                return device
        return None


class Graph:
    """æ¨¡æ‹Ÿè®¡ç®—å›¾æ¨¡å—"""
    
    @staticmethod
    def from_torch_model(torch_model) -> ModelGraph:
        """ä» PyTorch æ¨¡å‹åˆ›å»ºè®¡ç®—å›¾"""
        print("ğŸ”„ [æ¨¡æ‹Ÿ] è½¬æ¢ PyTorch æ¨¡å‹ä¸º MAX è®¡ç®—å›¾...")
        time.sleep(1.0)  # æ¨¡æ‹Ÿè½¬æ¢æ—¶é—´
        
        # åˆ›å»ºæ¨¡æ‹Ÿå›¾
        graph = ModelGraph("converted_model")
        
        # ç®€åŒ–çš„è½¬æ¢é€»è¾‘
        if hasattr(torch_model, 'config'):
            config = torch_model.config
            num_layers = getattr(config, 'num_hidden_layers', 32)
            
            for i in range(num_layers):
                graph.add_layer(f"layer_{i}", {
                    "type": "transformer_block",
                    "hidden_size": getattr(config, 'hidden_size', 4096)
                })
        
        print("âœ… [æ¨¡æ‹Ÿ] æ¨¡å‹è½¬æ¢å®Œæˆ")
        return graph


# å…¨å±€å®ä¾‹
engine = Engine()
graph = Graph()

# æ¨¡æ‹Ÿ max æ¨¡å—çš„ç»“æ„
class MaxModule:
    def __init__(self):
        self.engine = engine
        self.graph = graph

# åˆ›å»ºæ¨¡å—å®ä¾‹ä»¥æ”¯æŒ from max import engine, graph
max_module = MaxModule()
EOF

echo "âœ… æ¨¡æ‹Ÿç¯å¢ƒåˆ›å»ºå®Œæˆ"

# åˆ›å»ºæµ‹è¯•è„šæœ¬
echo "ğŸ§ª åˆ›å»ºæµ‹è¯•è„šæœ¬..."
cat > scripts/test_engine.py << 'EOF'
#!/usr/bin/env python3
"""
WiCore Mojo æ¨ç†å¼•æ“æµ‹è¯•è„šæœ¬
éªŒè¯æ‰€æœ‰æ ¸å¿ƒç»„ä»¶çš„åŠŸèƒ½å’Œé›†æˆ
"""

import sys
import os
import time
import json
import subprocess
from typing import Dict, Any

# æ·»åŠ æ¨¡æ‹Ÿç¯å¢ƒåˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'simulation'))

def test_environment():
    """æµ‹è¯•åŸºç¡€ç¯å¢ƒ"""
    print("ğŸ”§ æµ‹è¯•ç¯å¢ƒé…ç½®...")
    
    # æµ‹è¯• Python ç‰ˆæœ¬
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    
    # æµ‹è¯•åŸºç¡€åŒ…
    try:
        import torch
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"âœ… GPU æ•°é‡: {torch.cuda.device_count()}")
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return False
    
    try:
        import transformers
        print(f"âœ… Transformers: {transformers.__version__}")
    except ImportError:
        print("âŒ Transformers æœªå®‰è£…")
        return False
    
    return True

def test_modular_integration():
    """æµ‹è¯• Modular é›†æˆ"""
    print("\nğŸ”§ æµ‹è¯• Modular é›†æˆ...")
    
    try:
        # å°è¯•å¯¼å…¥æ¨¡æ‹Ÿçš„ max æ¨¡å—
        import max_simulation as max_sim
        engine = max_sim.engine
        
        print("âœ… MAX Engine æ¨¡æ‹Ÿç¯å¢ƒå¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•è®¾å¤‡å‘ç°
        devices = engine.discover_devices()
        print(f"âœ… å‘ç° {len(devices)} ä¸ªè®¾å¤‡")
        for device in devices:
            print(f"   - {device.type}: {device.id}")
        
        return True
    except Exception as e:
        print(f"âŒ Modular é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_configuration():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\nğŸ“‹ æµ‹è¯•é…ç½®æ–‡ä»¶...")
    
    try:
        with open('configs/production.json', 'r') as f:
            prod_config = json.load(f)
        print("âœ… ç”Ÿäº§é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        with open('configs/development.json', 'r') as f:
            dev_config = json.load(f)
        print("âœ… å¼€å‘é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
        
        return True
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_pixi_environment():
    """æµ‹è¯• Pixi ç¯å¢ƒ"""
    print("\nğŸ“¦ æµ‹è¯• Pixi ç¯å¢ƒ...")
    
    try:
        # æ£€æŸ¥ pixi.toml æ–‡ä»¶
        if os.path.exists('pixi.toml'):
            print("âœ… pixi.toml æ–‡ä»¶å­˜åœ¨")
        else:
            print("âš ï¸  pixi.toml æ–‡ä»¶ä¸å­˜åœ¨")
        
        # æµ‹è¯• pixi å‘½ä»¤
        result = subprocess.run(['pixi', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"âœ… Pixi ç‰ˆæœ¬: {result.stdout.strip()}")
            return True
        else:
            print("âŒ Pixi å‘½ä»¤ä¸å¯ç”¨")
            return False
    except Exception as e:
        print(f"âŒ Pixi ç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ WiCore Mojo æ¨ç†å¼•æ“æµ‹è¯•...")
    print("=" * 50)
    
    tests = [
        ("ç¯å¢ƒé…ç½®", test_environment),
        ("Modular é›†æˆ", test_modular_integration),
        ("é…ç½®æ–‡ä»¶", test_configuration),
        ("Pixi ç¯å¢ƒ", test_pixi_environment),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ¯ æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼WiCore ç¯å¢ƒé…ç½®æˆåŠŸ")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        return 1

if __name__ == "__main__":
    sys.exit(main())
EOF

chmod +x scripts/test_engine.py

echo "âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ"

# æ˜¾ç¤ºæ€»ç»“ä¿¡æ¯
echo ""
echo "ğŸ‰ WiCore Mojo æ¨ç†å¼•æ“ç¯å¢ƒæ­å»ºå®Œæˆï¼"
echo "=" * 50
echo "ğŸ¯ ç›®æ ‡è®¾å¤‡: $TARGET_DEVICES"
echo "ğŸ“¦ åŒ…ç®¡ç†å™¨: Pixi (å®˜æ–¹æ¨è)"
echo "ğŸ“„ é…ç½®æ–‡ä»¶: configs/production.json, configs/development.json"
echo ""
echo "ğŸš€ ä¸‹ä¸€æ­¥æ“ä½œ:"
echo "   1. æµ‹è¯•ç¯å¢ƒ: pixi run python scripts/test_engine.py"
echo "   2. ä¸‹è½½æ¨¡å‹: ä¸‹è½½ Gemma-3-27B åˆ° models/ ç›®å½•"
echo "   3. æ„å»ºé¡¹ç›®: pixi run ./scripts/build.sh"
echo "   4. å¯åŠ¨å¼•æ“: pixi run python src/wicore_engine.py"
echo ""
echo "ğŸ’¡ Pixi å¸¸ç”¨å‘½ä»¤:"
echo "   - pixi run <command>    # åœ¨ç¯å¢ƒä¸­è¿è¡Œå‘½ä»¤"
echo "   - pixi shell           # è¿›å…¥ç¯å¢ƒ shell"
echo "   - pixi add <package>   # æ·»åŠ åŒ…"
echo "   - pixi update          # æ›´æ–°åŒ…"
echo ""
echo "ğŸ“š æ–‡æ¡£ä½ç½®:"
echo "   - API æ–‡æ¡£: docs/API.md"
echo "   - æ€§èƒ½ä¼˜åŒ–: docs/PERFORMANCE.md"
echo "   - éƒ¨ç½²æŒ‡å—: docs/DEPLOYMENT.md" 