#!/bin/bash
# WiCore C++ æŽ¨ç†å¼•æ“Ž - ä¸€é”®éƒ¨ç½²è„šæœ¬
# é€‚ç”¨äºŽ Ubuntu 22.04 + NVIDIA GPU

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥rootæƒé™
check_root() {
    if [[ $EUID -ne 0 ]]; then
        log_error "æ­¤è„šæœ¬éœ€è¦rootæƒé™è¿è¡Œã€‚è¯·ä½¿ç”¨: sudo $0"
        exit 1
    fi
}

# æ£€æŸ¥ç³»ç»Ÿç‰ˆæœ¬
check_system() {
    log_info "æ£€æŸ¥ç³»ç»Ÿç‰ˆæœ¬..."
    
    if ! grep -q "Ubuntu 22.04" /etc/os-release; then
        log_warning "å»ºè®®ä½¿ç”¨ Ubuntu 22.04 LTS"
    fi
    
    log_success "ç³»ç»Ÿæ£€æŸ¥å®Œæˆ"
}

# æ£€æŸ¥GPU
check_gpu() {
    log_info "æ£€æŸ¥NVIDIA GPU..."
    
    if ! lspci | grep -i nvidia >/dev/null; then
        log_error "æœªæ£€æµ‹åˆ°NVIDIA GPU"
        exit 1
    fi
    
    gpu_info=$(lspci | grep -i nvidia | head -1)
    log_success "æ£€æµ‹åˆ°GPU: $gpu_info"
}

# æ›´æ–°ç³»ç»Ÿ
update_system() {
    log_info "æ›´æ–°ç³»ç»ŸåŒ…..."
    apt update && apt upgrade -y
    apt install -y wget curl git build-essential cmake ninja-build \
        pkg-config software-properties-common gpg-agent
    log_success "ç³»ç»Ÿæ›´æ–°å®Œæˆ"
}

# å®‰è£…NVIDIAé©±åŠ¨
install_nvidia_driver() {
    log_info "å®‰è£…NVIDIAé©±åŠ¨..."
    
    # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
    if nvidia-smi >/dev/null 2>&1; then
        log_success "NVIDIAé©±åŠ¨å·²å®‰è£…"
        return
    fi
    
    # æ·»åŠ NVIDIAä»“åº“
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
    dpkg -i cuda-keyring_1.0-1_all.deb
    apt update
    
    # å®‰è£…é©±åŠ¨
    apt install -y nvidia-driver-535 nvidia-dkms-535
    
    log_warning "NVIDIAé©±åŠ¨å®‰è£…å®Œæˆï¼Œè¯·é‡å¯ç³»ç»ŸåŽç»§ç»­"
    log_warning "é‡å¯åŽè¿è¡Œ: nvidia-smi éªŒè¯å®‰è£…"
    
    read -p "æ˜¯å¦çŽ°åœ¨é‡å¯? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        reboot
    fi
}

# å®‰è£…CUDA
install_cuda() {
    log_info "å®‰è£…CUDA 12.2..."
    
    # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…
    if nvcc --version >/dev/null 2>&1; then
        log_success "CUDAå·²å®‰è£…"
        return
    fi
    
    # ä¸‹è½½å¹¶å®‰è£…CUDA
    CUDA_INSTALLER="cuda_12.2.0_535.54.03_linux.run"
    if [[ ! -f $CUDA_INSTALLER ]]; then
        wget https://developer.download.nvidia.com/compute/cuda/12.2.0/local_installers/$CUDA_INSTALLER
    fi
    
    sh $CUDA_INSTALLER --silent --toolkit
    
    # æ·»åŠ çŽ¯å¢ƒå˜é‡
    cat >> /etc/environment << EOF
PATH="/usr/local/cuda-12.2/bin:$PATH"
LD_LIBRARY_PATH="/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH"
EOF
    
    # æ·»åŠ åˆ°å½“å‰session
    export PATH=/usr/local/cuda-12.2/bin:$PATH
    export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH
    
    log_success "CUDAå®‰è£…å®Œæˆ"
}

# å®‰è£…TensorRT
install_tensorrt() {
    log_info "å®‰è£…TensorRT..."
    
    # é€šè¿‡APTå®‰è£…
    apt install -y tensorrt libnvinfer-plugin8 libnvonnxparsers8
    
    # å®‰è£…Python bindings (å¯é€‰)
    apt install -y python3-libnvinfer python3-libnvinfer-dev
    
    log_success "TensorRTå®‰è£…å®Œæˆ"
}

# å®‰è£…ä¾èµ–åº“
install_dependencies() {
    log_info "å®‰è£…ä¾èµ–åº“..."
    
    apt install -y \
        libopencv-dev \
        libjsoncpp-dev \
        libevhtp-dev \
        libevent-dev \
        libsentencepiece-dev \
        protobuf-compiler \
        libprotobuf-dev \
        python3-pip \
        apache2-utils
    
    # å®‰è£…PythonåŒ…
    pip3 install huggingface-hub transformers torch
    
    log_success "ä¾èµ–åº“å®‰è£…å®Œæˆ"
}

# æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬
check_dependencies() {
    log_info "æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬..."
    
    # æ£€æŸ¥ç‰ˆæœ¬
    opencv_version=$(pkg-config --modversion opencv4 2>/dev/null || echo "æœªæ‰¾åˆ°")
    jsoncpp_version=$(pkg-config --modversion jsoncpp 2>/dev/null || echo "æœªæ‰¾åˆ°")
    
    log_info "OpenCVç‰ˆæœ¬: $opencv_version"
    log_info "JsonCppç‰ˆæœ¬: $jsoncpp_version"
    
    # éªŒè¯CUDAå’ŒTensorRT
    if nvcc --version >/dev/null 2>&1; then
        cuda_version=$(nvcc --version | grep "release" | awk '{print $6}' | cut -c2-)
        log_info "CUDAç‰ˆæœ¬: $cuda_version"
    fi
    
    if python3 -c "import tensorrt" >/dev/null 2>&1; then
        trt_version=$(python3 -c "import tensorrt as trt; print(trt.__version__)")
        log_info "TensorRTç‰ˆæœ¬: $trt_version"
    fi
    
    log_success "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# åˆ›å»ºå·¥ä½œç›®å½•
setup_workspace() {
    log_info "åˆ›å»ºå·¥ä½œç›®å½•..."
    
    WORK_DIR="/opt/wicore_deployment"
    mkdir -p $WORK_DIR/{models,logs,static}
    chmod 755 $WORK_DIR
    
    # åˆ›å»ºç¼“å­˜ç›®å½•
    mkdir -p /tmp/wicore_cache
    chmod 777 /tmp/wicore_cache
    
    log_success "å·¥ä½œç›®å½•åˆ›å»ºå®Œæˆ: $WORK_DIR"
    echo "WORK_DIR=$WORK_DIR" > /etc/environment
}

# ä¸‹è½½æ¨¡åž‹
download_model() {
    log_info "ä¸‹è½½Gemma-3-27B-ITæ¨¡åž‹..."
    
    WORK_DIR="/opt/wicore_deployment"
    MODEL_DIR="$WORK_DIR/models/gemma-3-27b-it"
    
    if [[ -d $MODEL_DIR && -f $MODEL_DIR/config.json ]]; then
        log_success "æ¨¡åž‹å·²å­˜åœ¨"
        return
    fi
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´ (éœ€è¦çº¦80GB)
    available_space=$(df $WORK_DIR | tail -1 | awk '{print $4}')
    required_space=$((80 * 1024 * 1024))  # 80GB in KB
    
    if [[ $available_space -lt $required_space ]]; then
        log_error "ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œéœ€è¦è‡³å°‘80GBå¯ç”¨ç©ºé—´"
        exit 1
    fi
    
    log_info "å¼€å§‹ä¸‹è½½æ¨¡åž‹ (çº¦50-60GB)ï¼Œè¯·è€å¿ƒç­‰å¾…..."
    
    # ä½¿ç”¨huggingface-cliä¸‹è½½
    su - $SUDO_USER -c "
        cd $WORK_DIR
        huggingface-cli download google/gemma-3-27b-it \
            --local-dir ./models/gemma-3-27b-it \
            --local-dir-use-symlinks False
    " || {
        log_warning "Hugging Faceä¸‹è½½å¤±è´¥ï¼Œå°è¯•git clone..."
        su - $SUDO_USER -c "
            cd $WORK_DIR
            git lfs install
            git clone https://huggingface.co/google/gemma-3-27b-it ./models/gemma-3-27b-it
        "
    }
    
    # åˆ›å»ºå¼•æ“Žç¼“å­˜ç›®å½•
    mkdir -p $MODEL_DIR/engine_cache
    chmod 755 $MODEL_DIR/engine_cache
    
    log_success "æ¨¡åž‹ä¸‹è½½å®Œæˆ"
}

# ç”Ÿæˆé…ç½®æ–‡ä»¶
generate_config() {
    log_info "ç”Ÿæˆé…ç½®æ–‡ä»¶..."
    
    WORK_DIR="/opt/wicore_deployment"
    CONFIG_FILE="$WORK_DIR/production_config.json"
    
    # è‡ªåŠ¨æ£€æµ‹GPUå†…å­˜
    gpu_memory=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    gpu_memory_gb=$((gpu_memory / 1024 - 4))  # é¢„ç•™4GB
    
    # è‡ªåŠ¨æ£€æµ‹CPUå†…å­˜
    cpu_memory=$(free -g | awk 'NR==2{print $2}')
    cpu_memory_gb=$((cpu_memory - 8))  # é¢„ç•™8GB
    
    cat > $CONFIG_FILE << EOF
{
  "model_path": "$WORK_DIR/models/gemma-3-27b-it",
  "tokenizer_path": "$WORK_DIR/models/gemma-3-27b-it/tokenizer.model",
  "server_port": 8080,
  "max_batch_size": 16,
  "max_context_length": 131072,
  "max_concurrent_requests": 32,
  
  "gpu_memory_gb": $gpu_memory_gb,
  "cpu_memory_gb": $cpu_memory_gb,
  "nvme_cache_path": "/tmp/wicore_cache",
  
  "trt_precision": "fp16",
  "trt_max_workspace_gb": 8,
  "trt_enable_sparse": true,
  "trt_enable_refit": true,
  
  "image_resolution": 896,
  "image_preprocessing_threads": 4,
  "max_images_per_request": 10,
  
  "dynamic_batching": true,
  "batch_timeout_ms": 10,
  "request_timeout_seconds": 300,
  
  "enable_performance_logging": true,
  "performance_log_interval_seconds": 60,
  "log_level": "info"
}
EOF
    
    log_success "é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ: $CONFIG_FILE"
    log_info "GPUå†…å­˜é…ç½®: ${gpu_memory_gb}GB"
    log_info "CPUå†…å­˜é…ç½®: ${cpu_memory_gb}GB"
}

# åˆ›å»ºsystemdæœåŠ¡
create_service() {
    log_info "åˆ›å»ºsystemdæœåŠ¡..."
    
    WORK_DIR="/opt/wicore_deployment"
    SERVICE_USER=${SUDO_USER:-"wicore"}
    
    cat > /etc/systemd/system/wicore.service << EOF
[Unit]
Description=WiCore Inference Engine
After=network.target

[Service]
Type=simple
User=$SERVICE_USER
Group=$SERVICE_USER
WorkingDirectory=$WORK_DIR
ExecStart=$WORK_DIR/wicore_src/build/wicore_server $WORK_DIR/production_config.json
Restart=always
RestartSec=10
Environment=CUDA_VISIBLE_DEVICES=0
Environment=OMP_NUM_THREADS=8
Environment=LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/lib

[Install]
WantedBy=multi-user.target
EOF
    
    systemctl daemon-reload
    systemctl enable wicore
    
    log_success "systemdæœåŠ¡åˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºæµ‹è¯•è„šæœ¬
create_test_scripts() {
    log_info "åˆ›å»ºæµ‹è¯•è„šæœ¬..."
    
    WORK_DIR="/opt/wicore_deployment"
    
    # APIæµ‹è¯•è„šæœ¬
    cat > $WORK_DIR/test_api.sh << 'EOF'
#!/bin/bash
echo "=== WiCore APIæµ‹è¯• ==="

echo "1. å¥åº·æ£€æŸ¥"
curl -s http://localhost:8080/health | jq .

echo -e "\n2. æ¨¡åž‹åˆ—è¡¨"
curl -s http://localhost:8080/v1/models | jq .

echo -e "\n3. ç®€å•å¯¹è¯æµ‹è¯•"
curl -s -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma-3-27b-it",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ],
    "max_tokens": 100,
    "temperature": 0.7
  }' | jq .

echo -e "\næµ‹è¯•å®Œæˆ!"
EOF
    
    # æ€§èƒ½æµ‹è¯•è„šæœ¬
    cat > $WORK_DIR/benchmark.py << 'EOF'
#!/usr/bin/env python3
import time
import requests
import statistics
import concurrent.futures

def single_request():
    start_time = time.time()
    try:
        response = requests.post('http://localhost:8080/v1/chat/completions', 
            json={
                "model": "gemma-3-27b-it",
                "messages": [{"role": "user", "content": "Count from 1 to 10"}],
                "max_tokens": 100
            }, timeout=30)
        
        latency = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            tokens = data.get('usage', {}).get('completion_tokens', 0)
            return True, latency, tokens
        else:
            return False, latency, 0
    except Exception as e:
        return False, time.time() - start_time, 0

def run_benchmark(num_requests=20, concurrency=1):
    print(f"è¿è¡ŒåŸºå‡†æµ‹è¯•: {num_requests} è¯·æ±‚, å¹¶å‘åº¦: {concurrency}")
    
    results = []
    
    if concurrency == 1:
        # ä¸²è¡Œæµ‹è¯•
        for i in range(num_requests):
            success, latency, tokens = single_request()
            results.append((success, latency, tokens))
            print(f"è¯·æ±‚ {i+1}: {'æˆåŠŸ' if success else 'å¤±è´¥'} {latency:.2f}s {tokens} tokens")
    else:
        # å¹¶å‘æµ‹è¯•
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [executor.submit(single_request) for _ in range(num_requests)]
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                success, latency, tokens = future.result()
                results.append((success, latency, tokens))
                print(f"è¯·æ±‚ {i+1}: {'æˆåŠŸ' if success else 'å¤±è´¥'} {latency:.2f}s {tokens} tokens")
    
    # ç»Ÿè®¡ç»“æžœ
    successful = [r for r in results if r[0]]
    if successful:
        latencies = [r[1] for r in successful]
        token_counts = [r[2] for r in successful]
        
        avg_latency = statistics.mean(latencies)
        avg_tokens = statistics.mean(token_counts)
        total_tokens = sum(token_counts)
        total_time = sum(latencies)
        
        print(f"\n=== åŸºå‡†æµ‹è¯•ç»“æžœ ===")
        print(f"æˆåŠŸè¯·æ±‚: {len(successful)}/{num_requests}")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s")
        print(f"å¹³å‡Tokenæ•°: {avg_tokens:.1f}")
        print(f"æ€»ä½“åžåé‡: {total_tokens/total_time:.1f} tokens/s")
        print(f"å•è¯·æ±‚åžåé‡: {avg_tokens/avg_latency:.1f} tokens/s")

if __name__ == "__main__":
    print("å¼€å§‹æ€§èƒ½æµ‹è¯•...")
    run_benchmark(num_requests=10, concurrency=1)
EOF
    
    chmod +x $WORK_DIR/test_api.sh
    chmod +x $WORK_DIR/benchmark.py
    
    log_success "æµ‹è¯•è„šæœ¬åˆ›å»ºå®Œæˆ"
}

# æ˜¾ç¤ºéƒ¨ç½²æ‘˜è¦
show_summary() {
    WORK_DIR="/opt/wicore_deployment"
    
    echo
    log_success "=== WiCoreéƒ¨ç½²å®Œæˆ! ==="
    echo
    echo "ðŸ“ å·¥ä½œç›®å½•: $WORK_DIR"
    echo "âš™ï¸  é…ç½®æ–‡ä»¶: $WORK_DIR/production_config.json"
    echo "ðŸ“ æ—¥å¿—ç›®å½•: $WORK_DIR/logs"
    echo "ðŸ§ª æµ‹è¯•è„šæœ¬: $WORK_DIR/test_api.sh"
    echo "ðŸ“Š æ€§èƒ½æµ‹è¯•: $WORK_DIR/benchmark.py"
    echo
    echo "ðŸš€ ä¸‹ä¸€æ­¥æ“ä½œ:"
    echo "1. ç¼–è¯‘é¡¹ç›®:"
    echo "   cd $WORK_DIR/wicore_src && mkdir -p build && cd build"
    echo "   cmake .. && make -j\$(nproc)"
    echo
    echo "2. å¯åŠ¨æœåŠ¡:"
    echo "   sudo systemctl start wicore"
    echo "   sudo systemctl status wicore"
    echo
    echo "3. æµ‹è¯•API:"
    echo "   cd $WORK_DIR && ./test_api.sh"
    echo
    echo "4. æ€§èƒ½æµ‹è¯•:"
    echo "   cd $WORK_DIR && python3 benchmark.py"
    echo
    echo "5. æŸ¥çœ‹æ—¥å¿—:"
    echo "   sudo journalctl -u wicore -f"
    echo
    log_warning "æ³¨æ„: è¯·ç¡®ä¿å°†WiCoreæºä»£ç ä¸Šä¼ åˆ° $WORK_DIR/wicore_src/"
}

# ä¸»å‡½æ•°
main() {
    echo "========================================"
    echo "WiCore C++ æŽ¨ç†å¼•æ“Ž - ä¸€é”®éƒ¨ç½²è„šæœ¬"
    echo "é€‚ç”¨äºŽ Ubuntu 22.04 + NVIDIA GPU"
    echo "========================================"
    echo
    
    # æ£€æŸ¥æƒé™
    check_root
    
    # ä¿å­˜åŽŸç”¨æˆ·
    if [[ -z $SUDO_USER ]]; then
        log_error "è¯·ä½¿ç”¨sudoè¿è¡Œæ­¤è„šæœ¬"
        exit 1
    fi
    
    # ç³»ç»Ÿæ£€æŸ¥
    check_system
    check_gpu
    
    # è¯¢é—®ç”¨æˆ·è¦æ‰§è¡Œçš„æ­¥éª¤
    echo "è¯·é€‰æ‹©è¦æ‰§è¡Œçš„æ­¥éª¤ (å¯å¤šé€‰ï¼Œç”¨ç©ºæ ¼åˆ†éš”):"
    echo "1) æ›´æ–°ç³»ç»Ÿ"
    echo "2) å®‰è£…NVIDIAé©±åŠ¨"
    echo "3) å®‰è£…CUDA"
    echo "4) å®‰è£…TensorRT"
    echo "5) å®‰è£…ä¾èµ–åº“"
    echo "6) è®¾ç½®å·¥ä½œç›®å½•"
    echo "7) ä¸‹è½½æ¨¡åž‹"
    echo "8) ç”Ÿæˆé…ç½®"
    echo "9) åˆ›å»ºæœåŠ¡"
    echo "a) å…¨éƒ¨æ‰§è¡Œ"
    echo
    read -p "è¯·è¾“å…¥é€‰æ‹© (é»˜è®¤: a): " choices
    choices=${choices:-a}
    
    # æ‰§è¡Œé€‰æ‹©çš„æ­¥éª¤
    if [[ $choices == *"a"* || $choices == *"1"* ]]; then
        update_system
    fi
    
    if [[ $choices == *"a"* || $choices == *"2"* ]]; then
        install_nvidia_driver
    fi
    
    if [[ $choices == *"a"* || $choices == *"3"* ]]; then
        install_cuda
    fi
    
    if [[ $choices == *"a"* || $choices == *"4"* ]]; then
        install_tensorrt
    fi
    
    if [[ $choices == *"a"* || $choices == *"5"* ]]; then
        install_dependencies
        check_dependencies
    fi
    
    if [[ $choices == *"a"* || $choices == *"6"* ]]; then
        setup_workspace
    fi
    
    if [[ $choices == *"a"* || $choices == *"7"* ]]; then
        download_model
    fi
    
    if [[ $choices == *"a"* || $choices == *"8"* ]]; then
        generate_config
    fi
    
    if [[ $choices == *"a"* || $choices == *"9"* ]]; then
        create_service
        create_test_scripts
    fi
    
    # æ˜¾ç¤ºæ‘˜è¦
    show_summary
}

# é”™è¯¯å¤„ç†
trap 'log_error "è„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œè¯·æŸ¥çœ‹é”™è¯¯ä¿¡æ¯"; exit 1' ERR

# æ‰§è¡Œä¸»å‡½æ•°
main "$@" 